---
title: "Comparison of Ensemble Recalibration Methods in Flu Forecasting"
author: "Nutcha Wattanachit"
date: "12/14/2020"
header-includes:
   - \usepackage{booktabs}
   - \usepackage[utf8]{inputenc}
   - \usepackage{tabularx}
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{hyperref}
   - \usepackage{multicol}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage{makecell}
   - \usepackage{xcolor}
output:
  pdf_document:
        keep_tex: true
        latex_engine: xelatex
        extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
library(pipeR)
library(cdcfluview)
library(Matrix)
library(tidyverse)
library(cowplot)
library(data.table)
library(reshape2)
library(gridExtra)
library(ranger)
library(splines)
library(ggplot2)
library(xtable)
library(here)
library(stats)
library(mixtools)
library(cowplot)
library(ggforce)
library(grid)
library(rmutil)
library(here)
library(knitr)
library(kableExtra)
library(rstan)

knitr::opts_chunk$set(echo = FALSE, warning(file = "./R-warnings.txt"),
                      comment = FALSE, message=FALSE, fig.show= 'hold', fig.pos="h",
                      table.placement='h')
options(kableExtra.latex.load_packages = FALSE)
```

We compare 1) the equally-weighted ensemble, 2) the traditional linear pool (TLP), 3) the beta-transform linear pool (BLP), 4) the equally-weighted beta-transform linear pool, 5) the Bayesian finite beta mixture 6) the Bayesian finite beta mixture with equally-weighted component forecasts in the simulation studies and in the application of influenza forecasting. For both beta mixture approaches, the number of mixing beta components are $K=2,3,$ and $4$. 

# Methods

Let $f_1,...,f_M$ be predictive density forecasts from $M$ component forecasting models, the ensemble methods combine the component forecasting models as follows

## Equally-weighted ensemble (EW)

The equally-weighted ensemble combines the component forecasting models with the aggregation predictive distribution function

\begin{align}
f_{\text{EW}}(y)=\sum_{m=1}^M \frac{1}{M}f_m(y).
\end{align}

 
## Traditional linear pool (TLP)

The TLP finds a set of optimal nonnegative weights $w_i$ that maximize the likelihood of the aggregation predictive distribution function

\begin{align}
f_{\text{TLP}}(y)=\sum_{m=1}^M w_mf_m(y),
\end{align}

where $\sum_{m=1}^M w_m=1$. The TLP is underdispersed when the component models are probabilistically calibrated.

## Beta-transform linear pool (BLP)

The BLP applies a beta transform on the combined predictive cumulative distribution function 

\begin{align}
F_{\text{BLP}}(y)=B_{\alpha,\beta}\Big(\sum_{m=1}^M w_m F_m(y)\Big),
\end{align}

Specifically, the BLP finds the transformation parameters $\alpha,\beta > 0$, and a set of nonnegative weights $w_m$ that maximize the likelihood of the aggregated predictive distribution function

\begin{align}
f_{\text{BLP}}(y)=\Big(\sum_{m=1}^M w_mf_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M w_m F_m(y)\Big),
\end{align}

where $b_{\alpha,\beta}$ denotes the beta density and $\sum_{m=1}^M w_m=1$.

## Equally-weighted beta-transform linear pool (EW-BLP)

The EW-BLP applies a beta transform on the equally-weighted ensemble and has the predictive cumulative distribution function

\begin{align}
F_{\text{EW-BLP}}(y)=B_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big),
\end{align}

The EW-BLP finds the transformation parameters $\alpha,\beta > 0$ that maximize the likelihood of the aggregated predictive distribution function

\begin{align}
f_{\text{EW-BLP}}(y)=\Big(\sum_{m=1}^M w_mf_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big).
\end{align}

## Bayesian finite beta mixture ($\text{BM}_k$)

The $\text{BM}_k$ extends the BLP method by using a finite beta mixture combination formula

\begin{align}
F_{\text{BM}_k}(y)=\sum_{k=1}^K w_kB_{\alpha,\beta}\Big(\sum_{m=1}^M \omega_{km} F_m(y)\Big),
\end{align}

where the vector $w_1,..., w_K$ comprises the beta mixture weights, $\alpha_1,..., \alpha_K$ and $\beta_1,..., \beta_K$ are beta calibration parameters, and for each beta component $\boldsymbol{\omega}_k=(\omega_{k1},..., \omega_{kM})$ comprises the beta component-specific set of component model weights. The pdf representation of the method is

\begin{align}
f_{\text{BM}_k}(y)=\sum_{k=1}^K w_k(\sum_{m=1}^M \omega_{km} f_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \omega_{km} F_m(y)\Big).
\end{align}

## Bayesian finite equally weighted beta mixture  ($\text{EW-BM}_k$)

The $\text{EW-BM}_k$ uses a finite beta mixture combination formula to combine an equally-weighted ensemble as follows

\begin{align}
F_{\text{EW-BM}_k}(y)=\sum_{k=1}^K w_kB_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big),
\end{align}

where the vector $w_1,..., w_K$ comprises the beta mixture weights and $\alpha_1,..., \alpha_K$ and $\beta_1,..., \beta_K$ are beta calibration parameters.

\begin{align}
f_{\text{EW-BM}_k}(y)=\sum_{k=1}^K w_k(\sum_{m=1}^M \frac{1}{M} f_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big).
\end{align}

# Simulation studies

## Scenario 1: Unbiased and calibrated components 

The data generating process for the observation $Y$ in the regression model is 

$$
Y = X_0+a_1X_1+a_2X_2+a_3X_3+ \epsilon, \\
\epsilon \sim N(0,1)
$$
where $a_1=1,a_2=1,$ and $a_3=1.1$, and $X_0,X_1,X_2,X_3,$ and $\epsilon$ are independent, standard normal random variables. The TLP's PITs are approximately beta distributed (underdispersed inverted U-shape) in this scenario, so BLP should be able to find optimal $\alpha$ and $\beta$ to adjust the PITs. Specifically, this scenario serves to demonstrate the shortcoming of TLP and to motivate BLP. We expect BMC to do as well as BLP as it is more flexible (and thus has higher complexity), but BMC is not necessary.  

```{r, include=FALSE}
set.seed(1234)
source("./applied_blp_sim.R")
# -----------Scenario 0: Simulation setup -----------------------#
n <- 1e5

coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))

Y       <- vars %*% coefs + rnorm(n = n)

means0 <- sds0 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means0)){
  ind <- 1:4 %in% c(1, i+1)
  means0[,i]  <- vars[, ind] %*% coefs[ind]
  sds0[,i]    <- sqrt(1 + sum(coefs[!ind]^2))
}

forecast_names <- c("TLP", "BLP","EW", "EW-BLP",
                    "BMC2","EW-BMC2","BMC3","EW-BMC3",
                    "BMC4","EW-BMC4","BMC5","EW-BMC5")

ind_est <- c(rep(TRUE, n-(n/5)), rep(FALSE, n/5))
```

```{r,fig.align="center",fig.pos='H',fig.width=5, fig.height=3,message=FALSE,warning=FALSE,fig.cap="",results='hide'}
plot(density(Y),main="Distribution of Y", cex.main=0.8)
```

The individual predictive densities have partial access of the above set of covariates. $f_1$ has access to only $X_0$ and $X_1$, $f_2$ has access to only $X_0$ and $X_2$, and $f_3$ has access to only $X_0$ and $X_3$. We want to combine $f_1,f_2,$ and $f_3$ to predict $Y$. In this setup, $X_0$ represent shared information, while other covariates represent information unique to each individual model. 

We estimate the pooling/combination formulas on a random sample ${(f_{1i} , f_{2i} , f_{3i}, Y_i) : i = 1,..., n}$ of size $n = 80,000$ and evaluate on an independent test sample of $n=20,000$. In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, so that $f_3$ is a more concentrated, sharper density forecast than $f_1$ and $f_2$ (Gneiting and Ranjan (2013)) and they are defined as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1,1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r, include=FALSE}
# make density forecasts and component PITs
comp_forecasts <- dnorm(Y, means0, sds0)
comp_pits <- pnorm(Y, means0, sds0)

# component
component_results_s1 <- get_component_res(comp_forecasts,comp_pits,ind_est, !ind_est,PIT=TRUE)

# TLP/BLP/EW/EW-BLP -----------------------------------
for(ens in forecast_names[1:4]){
  assign(paste0(ens,"_results_s1"), 
         make_freq_ensemble(ens,comp_forecasts,comp_pits, c(0.3, 0.3, 0.4),ind_est, !ind_est))
}

# BMC/EW-BMC --------------------------------------------------------------------------
for(mix_ens in forecast_names[5:12]){
  if(grepl(2,mix_ens, fixed = TRUE)){
    K<-2
    init_vals <- list(mu=rnorm(2,0.5,1e-3),nu=rnorm(2,2,1e-3),w=c(0.3,0.7))
    } else if(grepl(3,mix_ens, fixed = TRUE)){
      K<-3
      init_vals <- list(mu=rnorm(3,0.5,1e-3),nu=rnorm(3,2,1e-3),w=c(0.1,0.3,0.6))
      } else if(grepl(4,mix_ens, fixed = TRUE)){
        K<-4
        init_vals <- list(mu=rnorm(4,0.5,1e-3),nu=rnorm(4,2,1e-3),w=c(0.1,0.3,0.4,0.2))
      } else if(grepl(5,mix_ens, fixed = TRUE)){
        K<-5
        init_vals <- list(mu=rnorm(5,0.5,1e-3),nu=rnorm(5,2,1e-3),w=c(0.1,0.2,0.3,0.35,0.05))
      } else if(grepl(1,mix_ens, fixed = TRUE)){
        K<-1
        init_vals <- list(mu=rnorm(1,0.5,1e-3),nu=rnorm(1,2,1e-3),w=1.0)
        dim(init_vals$mu) <- 1
        dim(init_vals$nu) <- 1
        dim(init_vals$w) <- 1
        }
  # if(!(grepl(mix_ens,"EW",fixed = TRUE))){
  #   init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s1$params))
  # }
    if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC1",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s1$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC2",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s1$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC3",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC2_results_s1$params[5],
                                         BMC2_results_s1$params[7],
                                         BMC2_results_s1$params[9],
                                         BMC2_results_s1$params[6],
                                         BMC2_results_s1$params[8],
                                         BMC2_results_s1$params[10],
                                         TLP_results_s1$params),
                                       nrow=ncol(comp_forecasts),ncol=3))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC4",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC3_results_s1$params[7],
                                         BMC3_results_s1$params[10],
                                         BMC3_results_s1$params[13],
                                         BMC3_results_s1$params[8],
                                         BMC3_results_s1$params[11],
                                         BMC3_results_s1$params[14],
                                         BMC3_results_s1$params[9],
                                         BMC3_results_s1$params[12],
                                         BMC3_results_s1$params[15],
                                         TLP_results_s1$params),
                                       nrow=ncol(comp_forecasts),ncol=4))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC5",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC4_results_s1$params[9],
                                         BMC4_results_s1$params[13],
                                         BMC4_results_s1$params[17],
                                         BMC4_results_s1$params[10],
                                         BMC4_results_s1$params[14],
                                         BMC4_results_s1$params[18],
                                         BMC4_results_s1$params[11],
                                         BMC4_results_s1$params[15],
                                         BMC4_results_s1$params[19],
                                         BMC4_results_s1$params[12],
                                         BMC4_results_s1$params[16],
                                         BMC4_results_s1$params[20],
                                         TLP_results_s1$params),
                                       nrow=ncol(comp_forecasts),ncol=5))
  }
  hyperpars <- list(rep(1.0,K),rep(1.0,ncol(comp_forecasts)),rep(1/ncol(comp_forecasts),ncol(comp_forecasts)))
  names(hyperpars) <- c("alpha_w","alpha_omega","omega_ew")
  assign(paste0(mix_ens,"_results_s1"),
         make_mix_ensemble(mix_ens,K,comp_forecasts,comp_pits, init_vals,hyperpars,ind_est, !ind_est))
  }
```


```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------
table0 <-make_table(forecast_names[c(1:4)],"s1",comp_forecasts)
table01 <-make_table_bmc(forecast_names[5:12],"s1")
mls0 <- make_ls(forecast_names[1:12],"s1")
```

```{r, fig.align="center",message=FALSE,warning=FALSE,fig.cap=""}
# Overview over estimated parameters
knitr::kable(list(table0,table01[,1:5]), "latex", booktabs = T, 
             caption = "Model and Beta Mixing Weight Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table01[,6:15], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table01[,16:30], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(list(mls0[1:4,],mls0[5:12,]), "latex", booktabs = T,caption = "Log Score",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits,forecast_names[1:12],"s1",train=TRUE)
```

\clearpage

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits,forecast_names[1:12],"s1",train=FALSE)
```

\clearpage

<!-- # ```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""} -->
<!-- # # PDF -->
<!-- # plot_PDFs(Y,comp_forecasts,forecast_names[1:10],"s1") -->
<!-- # ``` -->
<!-- #  -->
<!-- # \clearpage -->

## Scenario 2: Multimodal DGP (Normal mixture) and close-$\mathcal{M}$
The data generating process for the observation $y_t$ is 

$$
y_t \overset{i.i.d.}{\sim} p_1\text{N}(-2,0.25)+p_2\text{N}(0,0.25)+p_3\text{N}(2,0.25), \\
t= 1,...,100,0000
$$

where $p_1=0.2,p_2=0.2$, and $p_3=0.6$. In this scenario, the three component models are in the data generating process and the TLP's PITs are approximately beta distributed (uniformly distributed, specifically). This scenario serves to show the situation in which TLP is an optimal method of combining forecast distributions. We expect BLP and BMC to perform as equally well as TLP with higher complexity. In other words, this is when BLP and BMC are not needed. 

```{r, include=FALSE}
p <- c(0.2,0.2,0.6)
mu <- c(-2,0,2)
sigma <- rep(0.25, 3)
Y <- rnormmix(n, p, mu, sigma)
```

```{r,fig.align="center",fig.pos='H',fig.width=5, fig.height=3,message=FALSE,warning=FALSE,fig.cap="",results='hide'}
plot(density(Y),main="Distribution of Y", cex.main=0.8)
```

The individual predictive densities are defined as follows:

$$
\begin{aligned}
f_{1}&\overset{i.i.d.}{\sim}\text{N}(-2,0.25)\\
f_{2}&\overset{i.i.d.}{\sim}\text{N}(0,0.25)\\
f_{3}&\overset{i.i.d.}{\sim}\text{N}(2,0.25)\\
\end{aligned}
$$


```{r, include=FALSE}
# make density forecasts and component PITs
comp_forecasts2 <- cbind(dnorm(Y, -2, 0.25),dnorm(Y, 0, 0.25),dnorm(Y, 2, 0.25))
comp_pits2 <- cbind(pnorm(Y, -2, 0.25),pnorm(Y, 0, 0.25),pnorm(Y, 2, 0.25))

# component
component_results_s2 <- get_component_res(comp_forecasts2,comp_pits2,ind_est, !ind_est,PIT=TRUE)
# TLP/BLP/EW/EW-BLP -----------------------------------
for(ens in forecast_names[1:4]){
  assign(paste0(ens,"_results_s2"), 
         make_freq_ensemble(ens,comp_forecasts2,comp_pits2, c(0.3, 0.3, 0.4),ind_est, !ind_est))
}

# BMC/EW-BMC --------------------------------------------------------------------------
for(mix_ens in forecast_names[5:12]){
  if(grepl(2,mix_ens, fixed = TRUE)){
    K<-2
    init_vals <- list(mu=rnorm(2,0.5,1e-3),nu=rnorm(2,2,1e-3),w=c(0.3,0.7))
    } else if(grepl(3,mix_ens, fixed = TRUE)){
      K<-3
      init_vals <- list(mu=rnorm(3,0.5,1e-3),nu=rnorm(3,2,1e-3),w=c(0.1,0.3,0.6))
      } else if(grepl(4,mix_ens, fixed = TRUE)){
        K<-4
        init_vals <- list(mu=rnorm(4,0.5,1e-3),nu=rnorm(4,2,1e-3),w=c(0.1,0.3,0.4,0.2))
      } else if(grepl(5,mix_ens, fixed = TRUE)){
        K<-5
        init_vals <- list(mu=rnorm(5,0.5,1e-3),nu=rnorm(5,2,1e-3),w=c(0.1,0.2,0.3,0.35,0.05))
      } else if(grepl(1,mix_ens, fixed = TRUE)){
        K<-1
        init_vals <- list(mu=rnorm(1,0.5,1e-3),nu=rnorm(1,2,1e-3),w=1.0)
        dim(init_vals$mu) <- 1
        dim(init_vals$nu) <- 1
        dim(init_vals$w) <- 1
        }
  # if(!(grepl(mix_ens,"EW",fixed = TRUE))){
  #   init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s2$params))
  # }
    if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC1",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s2$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC2",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s2$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC3",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC2_results_s2$params[5],
                                         BMC2_results_s2$params[7],
                                         BMC2_results_s2$params[9],
                                         BMC2_results_s2$params[6],
                                         BMC2_results_s2$params[8],
                                         BMC2_results_s2$params[10],
                                         TLP_results_s2$params),
                                       nrow=ncol(comp_forecasts2),ncol=3))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC4",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC3_results_s2$params[7],
                                         BMC3_results_s2$params[10],
                                         BMC3_results_s2$params[13],
                                         BMC3_results_s2$params[8],
                                         BMC3_results_s2$params[11],
                                         BMC3_results_s2$params[14],
                                         BMC3_results_s2$params[9],
                                         BMC3_results_s2$params[12],
                                         BMC3_results_s2$params[15],
                                         TLP_results_s2$params),
                                       nrow=ncol(comp_forecasts2),ncol=4))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC5",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC4_results_s2$params[9],
                                         BMC4_results_s2$params[13],
                                         BMC4_results_s2$params[17],
                                         BMC4_results_s2$params[10],
                                         BMC4_results_s2$params[14],
                                         BMC4_results_s2$params[18],
                                         BMC4_results_s2$params[11],
                                         BMC4_results_s2$params[15],
                                         BMC4_results_s2$params[19],
                                         BMC4_results_s2$params[12],
                                         BMC4_results_s2$params[16],
                                         BMC4_results_s2$params[20],
                                         TLP_results_s2$params),
                                       nrow=ncol(comp_forecasts2),ncol=5))
  }
  hyperpars <- list(rep(1.0,K),rep(1.0,ncol(comp_forecasts2)),rep(1/ncol(comp_forecasts2),ncol(comp_forecasts2)))
  names(hyperpars) <- c("alpha_w","alpha_omega","omega_ew")
  assign(paste0(mix_ens,"_results_s2"),
         make_mix_ensemble(mix_ens,K,comp_forecasts2,comp_pits2, init_vals,hyperpars,ind_est, !ind_est))
  }
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------
table2 <-make_table(forecast_names[c(1:4)],"s2",comp_forecasts2)
table21 <-make_table_bmc(forecast_names[5:12],"s2")
mls2 <- make_ls(forecast_names[1:12],"s2")
```

```{r, fig.align="center",message=FALSE,warning=FALSE,fig.cap=""}
# Overview over estimated parameters
knitr::kable(list(table2,table21[,1:5]), "latex", booktabs = T, 
             caption = "Model and Beta Mixing Weight Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table21[,6:15], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table21[,16:30], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(list(mls2[1:4,],mls2[5:12,]), "latex", booktabs = T,caption = "Log Score",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

\clearpage

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits2,forecast_names[1:12],"s2",train=TRUE)
```

\clearpage

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits2,forecast_names[1:12],"s2",train=FALSE)
```

\clearpage

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PDF
plot_PDFs(Y,comp_forecasts2,forecast_names[1:12],"s2",c(0,1.8))
```

\clearpage

## Scenario 3: Multimodal DGP (Normal mixture) and open-$\mathcal{M}$

The data generating process for the observations in this scenario is the same as in Scenario 2. There are two component models defined as follows

$$
\begin{aligned}
f_{1}&\overset{i.i.d.}{\sim}\text{N}(2,1)\\
f_{2}&\overset{i.i.d.}{\sim}\text{N}(-1,1).\\
\end{aligned}
$$

The component models are not part of the data generating process. In this scenario the TLP's PITs are not approximately beta distributed, so we expect BLP to not be able to find optimal $\alpha$ and $\beta$ to calibrate the PITs. Specifically, this scenario serves to motivate BMC and show that BMC is highly flexible and can calibrate the PITs when BLP cannot. We also expect BMC with higher K to be more flexible than BMC with lower K.  

```{r, include=FALSE}
# make density forecasts and component PITs
comp_forecasts3 <- cbind(dnorm(Y, 2, 1),dnorm(Y, -1, 1))
comp_pits3 <- cbind(pnorm(Y, 2, 1),pnorm(Y, -1, 1))

# TLP/BLP/EW/EW-BLP -----------------------------------
for(ens in forecast_names[1:4]){
  assign(paste0(ens,"_results_s3"), 
         make_freq_ensemble(ens,comp_forecasts3,comp_pits3, c(0.4, 0.6),ind_est, !ind_est))
}

# BMC/EW-BMC --------------------------------------------------------------------------
for(mix_ens in forecast_names[5:12]){
  if(grepl(2,mix_ens, fixed = TRUE)){
    K<-2
    init_vals <- list(mu=rnorm(2,0.5,1e-3),nu=rnorm(2,2,1e-3),w=c(0.3,0.7))
    } else if(grepl(3,mix_ens, fixed = TRUE)){
      K<-3
      init_vals <- list(mu=rnorm(3,0.5,1e-3),nu=rnorm(3,2,1e-3),w=c(0.1,0.3,0.6))
      } else if(grepl(4,mix_ens, fixed = TRUE)){
        K<-4
        init_vals <- list(mu=rnorm(4,0.5,1e-3),nu=rnorm(4,2,1e-3),w=c(0.1,0.3,0.4,0.2))
      } else if(grepl(5,mix_ens, fixed = TRUE)){
        K<-5
        init_vals <- list(mu=rnorm(5,0.5,1e-3),nu=rnorm(5,2,1e-3),w=c(0.1,0.2,0.3,0.35,0.05))
      } else if(grepl(1,mix_ens, fixed = TRUE)){
        K<-1
        init_vals <- list(mu=rnorm(1,0.5,1e-3),nu=rnorm(1,2,1e-3),w=1.0)
        dim(init_vals$mu) <- 1
        dim(init_vals$nu) <- 1
        dim(init_vals$w) <- 1
        }
  if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC1",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s3$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC2",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(replicate(K, TLP_results_s3$params))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC3",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC2_results_s3$params[5],
                                         BMC2_results_s3$params[7],
                                         BMC2_results_s3$params[6],
                                         BMC2_results_s3$params[8],
                                         TLP_results_s3$params),
                                       nrow=ncol(comp_forecasts3),ncol=3))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC4",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC3_results_s3$params[7],
                                         BMC3_results_s3$params[10],
                                         BMC3_results_s3$params[8],
                                         BMC3_results_s3$params[11],
                                         BMC3_results_s3$params[9],
                                         BMC3_results_s3$params[12],
                                         TLP_results_s3$params),
                                       nrow=ncol(comp_forecasts3),ncol=4))
  } else if(!(grepl(mix_ens,"EW",fixed = TRUE)) && grepl("BMC5",mix_ens,fixed = TRUE)){
    init_vals[["omega_k"]] <- t(matrix(c(BMC4_results_s3$params[9],
                                         BMC4_results_s3$params[13],
                                         BMC4_results_s3$params[10],
                                         BMC4_results_s3$params[14],
                                         BMC4_results_s3$params[11],
                                         BMC4_results_s3$params[15],
                                         BMC4_results_s3$params[12],
                                         BMC4_results_s3$params[16],
                                         TLP_results_s3$params),
                                       nrow=ncol(comp_forecasts3),ncol=5))
  }
  hyperpars <- list(rep(1.0,K),rep(1.0,ncol(comp_forecasts3)),rep(1/ncol(comp_forecasts3),ncol(comp_forecasts3)))
  names(hyperpars) <- c("alpha_w","alpha_omega","omega_ew")
  assign(paste0(mix_ens,"_results_s3"),
         make_mix_ensemble(mix_ens,K,comp_forecasts3,comp_pits3, init_vals,hyperpars,ind_est, !ind_est))
  }
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------
table3 <-make_table2(forecast_names[c(1:4)],"s3",comp_forecasts3)
table31 <-make_table_bmc2(forecast_names[5:12],"s3")
mls3 <- make_ls(forecast_names[1:12],"s3")
```

```{r, fig.align="center",message=FALSE,warning=FALSE,fig.cap=""}
# Overview over estimated parameters
knitr::kable(list(table3,table31[,1:5]), "latex", booktabs = T, 
             caption = "Model and Beta Mixing Weight Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table31[,6:15], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(table31[,16:25], "latex", booktabs = T,caption = "Mixture Parameters",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
knitr::kable(list(mls3[1:4,],mls3[5:12,]), "latex", booktabs = T,caption = "Log Score",escape=FALSE) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

\clearpage

```{r,fig.align="center",fig.width=6, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits3,forecast_names[1:12],"s3",train=TRUE)
```

\clearpage

```{r,fig.align="center",fig.width=6, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PIT Histogramme
plot_PITs(comp_pits3,forecast_names[1:12],"s3",train=FALSE)
```

\clearpage

```{r,fig.align="center",fig.width=7, fig.height=11,message=FALSE,warning=FALSE,fig.cap=""}
# PDF
plot_PDFs(Y,comp_forecasts3,forecast_names[1:12],"s3",c(0,1.2))
```

