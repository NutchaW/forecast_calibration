---
title: "BLP Simulation"
author: "Nutcha Wattanachit"
date: "06/01/2019"
header-includes:
   - \usepackage{booktabs}
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{multicol}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage{makecell}
   - \usepackage{xcolor}
output:
  pdf_document:
        keep_tex: true
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library("pipeR");library("dplyr");library("cdcfluview")
library(fGarch);library(Matrix)
library(quantreg);library(tidyverse);library(cowplot)
library(data.table);library(reshape2);library(gridExtra)
library(rmutil);library(ranger)
library(xgboost);library(splines);library(ggplot2)
library(xtable);library(here)
library(stats);library(mixtools);library(cowplot)
library(ggforce);library(grid);library(FluSight)
library(rmutil);library(here);library(stats)
library(mixtools);library(knitr);library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, comment = FALSE, message=FALSE, fig.show= 'hold',
                      table.placement='H')
options(kableExtra.latex.load_packages = FALSE)
```

# Ensemble/pooling Methods

## TLP

Traditional linear pool finds optimal weights that maximmizes the likelihood of $f(y)=\sum_{i=1}^k w_if_i(y)$.


## BLP

BLP finds $\alpha$, $\beta$, and weights that maximize the likelihood of 

$$g_{\alpha,\beta}=(\sum_{i=1}^k w_if_i(y))b_{\alpha,\beta}(\sum_{i=1}^k w_iF_i(y)).$$

 BLP Example: To obtain $\alpha$, $\beta$, and the weights for all component models, train the BLP model on half of the data. Then, use $\alpha$, $\beta$, and the weights from training to apply to the data held out for testing. 

## SLP


## BLP with Non-central Parameter(nBLP)

nBLP finds $\alpha$, $\beta$, non-central parameter $\lambda$, and weights that maximize the likelihood of 

$g_{\alpha,\beta,\lambda}=(\sum_{i=1}^k w_if_i(y))b_{\alpha,\beta}(\sum_{i=1}^k w_iF_i(y)).$

nBLP process:  To obtain $\alpha$, $\beta$, $\lambda$, and the weights for all component models, train the nBLP model on half of the data. Then, use $\alpha$, $\beta$, $\lambda$, and the weights from training to apply to the data held out for testing.
 
## Component-wise BLP (cBLP)

This is the extension of the traditional BLP. We beta-transform each of the cumulative distribution functions of the component models. This is done by finding $\alpha$ and $\beta$ that maximize the likelihood of 

$$
\begin{aligned}
G_{i,\alpha_i,\beta_i} &= B_{\alpha_i,\beta_i}[F_i(y)]\\
g_{i,\alpha_i,\beta_i} &= f_i(y) \times b_{\alpha_i,\beta_i}[F_i(y)]
\end{aligned}
$$

Then, to obtain $\alpha$, $\beta$, and the weights for 21 models, we apply BLP on the beta-transformed components: 

$$
\begin{aligned}
G_{\alpha,\beta} &= B_{\alpha,\beta}\Big[\sum_{i=1}^k w_iB_{\alpha_i,\beta_i}(F_i(y))\Big]\\
g_{\alpha,\beta} &= \Big(\sum_{i=1}^k w_ib_{i,\alpha_i,\beta_i}(F_i(y))f_i(y)  \Big)b_{\alpha,\beta} \Big(\sum_{i=1}^k w_iB_{i,\alpha_i,\beta_i}(F_i(y))\Big)
\end{aligned}
$$

cBLP - Part 1: For each component model, train over all observations to get $\alpha_i$ and $\beta_i$. Then, apply $\alpha_i$ and $\beta_i$ to beta-transform the CDF. This ends the component-wise part. 

cBLP Part 2: Apply the usual BLP process on the beta-transformed component models to get the BLP ensemble. 

## Componentwise Bias-Corrected & Componentwise Recalibrated BLP (cbcBLP) 

This method corrects for bias of the component models and also recalibrate them using beta transform. Then the BLP method is used to generate the ensemble. 

 
## Component-wise SLP (cSLP)

This is the extension of the traditional BLP. We beta-transform each of the cumulative distribution functions of the component models. This is done by finding $\alpha$ and $\beta$ that maximize the likelihood of 

$$
\begin{aligned}
G_{i,\alpha_i,\beta_i} &= B_{\alpha_i,\beta_i}[F_i(y)]\\
g_{i,\alpha_i,\beta_i} &= f_i(y) \times b_{\alpha_i,\beta_i}[F_i(y)]
\end{aligned}
$$

Then, to obtain $\alpha$, $\beta$, and the weights for 21 models, we apply BLP on the beta-transformed components: 

$$
\begin{aligned}
G_{\alpha,\beta} &= B_{\alpha,\beta}\Big[\sum_{i=1}^k w_iB_{\alpha_i,\beta_i}(F_i(y))\Big]\\
g_{\alpha,\beta} &= \Big(\sum_{i=1}^k w_ib_{i,\alpha_i,\beta_i}(F_i(y))f_i(y)  \Big)b_{\alpha,\beta} \Big(\sum_{i=1}^k w_iB_{i,\alpha_i,\beta_i}(F_i(y))\Big)
\end{aligned}
$$

cBLP - Part 1: For each component model, train over all observations to get $\alpha_i$ and $\beta_i$. Then, apply $\alpha_i$ and $\beta_i$ to beta-transform the CDF. This ends the component-wise part. 

cBLP Part 2: Apply the usual BLP process on the beta-transformed component models to get the BLP ensemble. 

## Componentwise Bias-Corrected & Componentwise Recalibrated BLP (cbcBLP) 

This method corrects for bias of the component models and also recalibrate them using beta transform. Then the BLP method is used to generate the ensemble. 

\clearpage

# Simulation studies

The data generating process for the observation $Y$ in the regression model is 

$$
Y=X_0+a_1X_1+a_2X_2+a_3X_3+\epsilon, \epsilon\sim(0,1)
$$

where $a_1,a_2,$ and $a_3$ are real constants that vary across different simulation studies, and $X_0,X_1,X_2,X_3,$ and $\epsilon$ are independent, standard normal random variables. The individual predictive densities have partial access of the above set of covariates. $f_1$ has access to only $X_0$ and $X_1$, $f_2$ has access to only $X_0$ and $X_2$, and $f_3$ has access to only $X_0$ and $X_3$. We want to combine $f_1,f_2,$ and $f_3$ to predict $Y$. In this setup, $X_0$ represent shared information, while other covariates represent information unique to each individual model.

We estimate the pooling/combination formulas on a random sample ${(f_{1i} , f_{2i} , f_{3i}, Y_i) : i = 1,..., n}$ of size $n = 50,000$ and evaluate on an independent test sample of the same size.

## Scenario 1: calibrated components (Baseline scenario).

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, so that $f_3$ is a more concentrated, sharper density forecast than $f_1$ and $f_2$ (Gneiting and Ranjan (2013)) and they are defined as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1,1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r, include=FALSE,cache=TRUE}
source("./new_simBLP.R")
# -----------Scenario 0: Simulation setup -----------------------#
n <- 1e5

coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))

Y       <- vars %*% coefs + rnorm(n = n)

means0 <- sds0 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means0)){
  ind <- 1:4 %in% c(1, i+1)
  means0[,i]  <- vars[, ind] %*% coefs[ind]
  sds0[,i]    <- sqrt(1 + sum(coefs[!ind]^2))
}

forecast_names <- c("TLP", "BLP", "SLP", "nBLP", "cBLP","cSLP")

ind_est <- c(rep(TRUE, n/2), rep(FALSE, n/2))

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP0<-get_TLPpars(means0,sds0)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP0<-get_BLPpars(means0,sds0)
weights_BLP0<-par_BLP0[-c(1:2)]
ab0<-par_BLP0[c(1:2)]

# SLP (spread-adjustment linear pool) combination -----------------------------------
par_SLP0<-get_SLPpars(means0,sds0)
weights_SLP0<-par_SLP0[-1]
c0<-par_SLP0[1]


# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP0<-get_nBLPpars(means0,sds0)
weights_nBLP0<-par_nBLP0[-c(1:3)]
abc0<-par_nBLP0[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars0<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab0<-get_indBLPpars(means0,sds0,i)
  comp_bpars0[i,1]<-ab0[1]
  comp_bpars0[i,2]<-ab0[2]
}

### BLP components in BLP
par_cBLP0<-get_cBLPpars(means0,sds0,comp_bpars0)
weights_cBLP0<-par_cBLP0[-c(1:2)]
cBLP_ab0<-par_cBLP0[c(1:2)]

# component-specific SLP combination ------------------------------
comp_c0<-matrix(NA,ncol=1,nrow=3)
for(i in 1:3){
  ci0<-get_indSLPpars(means0,sds0,i)
  comp_c0[i]<-ci0[1]}
### SLP components in SLP
par_cSLP0<-get_cSLPpars(means0,sds0,comp_c0)
weights_cSLP0<-par_cSLP0[-1]
cSLP_c0<-par_cSLP0[1]
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table0 <- matrix(NA, nrow = 6, ncol = 7, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp","c")))
table0[1,] <- c(weights_TLP0, rep(NA, 4))
table0[2,] <- c(par_BLP0[-c(1:2)], par_BLP0[1:2],NA,NA)
table0[3,] <- c(weights_SLP0, rep(NA, 3),c0)
table0[4,] <- c(par_nBLP0[-c(1:3)], par_nBLP0[1:3],NA)
table0[5,] <- c(par_cBLP0[-c(1:2)], par_cBLP0[1:2],NA,NA)
table0[6,] <- c(weights_cSLP0, rep(NA, 3),cSLP_c0)
table0<-round(table0, 3)

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls0 <- matrix(NA, nrow = 9, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls0[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means0[ind, i],
                                               sd = sds0[ind, i]))
   mls0[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], weights = weights_TLP0, eval = FALSE))
   mls0[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_BLP0, eval = FALSE))
   mls0[6, j] <- mean(wrapper_SLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_SLP0, eval = FALSE))
   mls0[7, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_nBLP0, eval = FALSE))
   mls0[8, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means0[ind,], sds = sds0[ind,],
                                      pars = par_cBLP0,cab=comp_bpars0, eval = FALSE))
   mls0[9, j] <- mean(wrapper_cSLP_nonc(Y = Y[ind], means = means0[ind,], 
                                           sds = sds0[ind,], pars = par_cSLP0,ind_con = comp_c0, eval = FALSE))
 }

var0 <- matrix(NA, nrow = 9, ncol = 1,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                "Variance" ))
 for(j in 1){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls0[i, j]  <- PIT_normal(Y = Y[ind], mu = means0[ind, i],
                                               sd = sds0[ind, i],var=TRUE)
   var0[4, j] <- wrapper_TLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], weights = weights_TLP0, eval = FALSE, varPIT=TRUE)
   var0[5, j] <- wrapper_BLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_BLP0, eval = FALSE, varPIT=TRUE)
   var0[6, j] <- wrapper_SLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_SLP0, eval = FALSE, varPIT=TRUE)
   var0[7, j] <- wrapper_nBLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_nBLP0, eval = FALSE, varPIT=TRUE)
   var0[8, j] <- wrapper_cBLP_nonc(Y = Y[ind], means = means0[ind,], sds = sds0[ind,],
                                      pars = par_cBLP0,cab=comp_bpars0, eval = FALSE, varPIT=TRUE)
   var0[9, j] <- wrapper_cSLP_nonc(Y = Y[ind], means = means0[ind,], 
                                           sds = sds0[ind,], pars = par_cSLP0,ind_con = comp_c0, eval = FALSE, varPIT=TRUE)
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
# Overview over estimated parameters
mls0<-round(mls0, 3)
var0<-round(var0, 3)
knitr::kable(list(table0,var0,mls0), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```


```{r,fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
# PIT Histogramme
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means0[!ind_est ,i], 
                         sd = sds0[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], weights = weights_TLP0, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_BLP0, plot = TRUE)
wrapper_SLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_SLP0, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_nBLP0, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_cBLP0, cab=comp_bpars0, plot = TRUE)
wrapper_cSLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_cSLP0, ind_con = comp_c0,plot = TRUE)
#dev.off()
```



\clearpage

## Scenario 2: Biased forecast scenario

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, and we add $N(2,1)$ to the mean of $f_1$ so that it is a biased forecast. The models are defined as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1+N(2,1),1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
#------------scene 1 Simulation setup ------------------------------------------------------------

coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))
noise<-rnorm(n,2,1)

Y       <- vars %*% coefs + rnorm(n = n)
means <- sds <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means)){
  ind <- 1:4 %in% c(1, i+1)
  means[,i]  <- vars[, ind] %*% coefs[ind]+noise*(i==1)
  sds[,i]    <- sqrt(1 + sum(coefs[!ind]^2))
}

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP2<-get_TLPpars(means,sds)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP2<-get_BLPpars(means,sds)
weights_BLP2<-par_BLP2[-c(1:2)]
ab2<-par_BLP2[c(1:2)]

# SLP (spread-adjustment linear pool) combination -----------------------------------
par_SLP2<-get_SLPpars(means,sds)
weights_SLP2<-par_SLP2[-1]
c2<-par_SLP2[1]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP2<-get_nBLPpars(means,sds)
weights_nBLP2<-par_nBLP2[-c(1:3)]
abc2<-par_nBLP2[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars2<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab2<-get_indBLPpars(means,sds,i)
  comp_bpars2[i,1]<-ab2[1]
  comp_bpars2[i,2]<-ab2[2]
}

# BLP components in BLP
par_cBLP2<-get_cBLPpars(means,sds,comp_bpars2)
weights_cBLP2<-par_cBLP2[-c(1:2)]
cBLP_ab2<-par_cBLP2[c(1:2)]

# component-specific SLP combination ------------------------------
comp_c2<-matrix(NA,ncol=1,nrow=3)
for(i in 1:3){
  ci2<-get_indSLPpars(means,sds,i)
  comp_c2[i]<-ci2[1]}
### SLP components in SLP
par_cSLP2<-get_cSLPpars(means,sds,comp_c2)
weights_cSLP2<-par_cSLP2[-1]
cSLP_c2<-par_cSLP2[1]
```


```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table2 <- matrix(NA, nrow = 6, ncol = 7, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp","c")))
table2[1,] <- c(weights_TLP2, rep(NA, 4))
table2[2,] <- c(par_BLP2[-c(1:2)], par_BLP2[1:2],NA,NA)
table2[3,] <- c(weights_bcTLP2, rep(NA, 3),c0)
table2[4,] <- c(par_bcBLP2[-c(1,2)], par_bcBLP2[1:2],NA)
table2[5,] <- c(par_nBLP2[-c(1:3)], par_nBLP2[1:3])
table2[6,] <- c(par_cBLP2[-c(1:2)], par_cBLP2[1:2],NA)

table2<-round(table2, 3)

table0[3,] <- c(weights_SLP0, rep(NA, 3),c0)
table0[4,] <- c(par_nBLP0[-c(1:3)], par_nBLP0[1:3],NA)
table0[5,] <- c(par_cBLP0[-c(1:2)], par_cBLP0[1:2],NA,NA)
table0[6,] <- c(weights_cSLP0, rep(NA, 3),cSLP_c0)

# get biased corrected forecast for test set
# use the coefficients from the train set so it's a+means*beta
bc_ests2<-bc_ests(Y, means)
bc_forecasts2_all<-cbind(bc_ests2[1,1]+(bc_ests2[1,2]*means[,1]),
                          bc_ests2[2,1]+(bc_ests2[2,2]*means[,2]),
                          bc_ests2[3,1]+(bc_ests2[3,2]*means[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls2 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls2[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means[ind, i],
                                               sd = sds[ind, i]))
   mls2[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], weights = weights_TLP2, eval = FALSE))
   mls2[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_BLP2, eval = FALSE))
   mls2[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], weights = weights_bcTLP2, eval = FALSE))
   mls2[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], pars = par_bcBLP2, eval = FALSE))
   mls2[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_nBLP2, eval = FALSE))
   mls2[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means[ind,], sds = sds[ind,],
                                      pars = par_cBLP2,cab=comp_bpars2, eval = FALSE))
   mls2[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,], 
                                           sds = sds[ind,], pars = par_cbcBLP2,
                                           cab=cbcomp_bpars2, eval = FALSE))
 }

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls2 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls2[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means[ind, i],
                                               sd = sds[ind, i]))
   mls2[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], weights = weights_TLP2, eval = FALSE))
   mls2[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_BLP2, eval = FALSE))
   mls2[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], weights = weights_bcTLP2, eval = FALSE))
   mls2[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], pars = par_bcBLP2, eval = FALSE))
   mls2[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_nBLP2, eval = FALSE))
   mls2[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means[ind,], sds = sds[ind,],
                                      pars = par_cBLP2,cab=comp_bpars2, eval = FALSE))
   mls2[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,], 
                                           sds = sds[ind,], pars = par_cbcBLP2,
                                           cab=cbcomp_bpars2, eval = FALSE))
 }

# Parameter values
table0 <- matrix(NA, nrow = 6, ncol = 7, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp","c")))
table0[1,] <- c(weights_TLP0, rep(NA, 4))
table0[2,] <- c(par_BLP0[-c(1:2)], par_BLP0[1:2],NA,NA)
table0[3,] <- c(weights_SLP0, rep(NA, 3),c0)
table0[4,] <- c(par_nBLP0[-c(1:3)], par_nBLP0[1:3],NA)
table0[5,] <- c(par_cBLP0[-c(1:2)], par_cBLP0[1:2],NA,NA)
table0[6,] <- c(weights_cSLP0, rep(NA, 3),cSLP_c0)
table0<-round(table0, 3)

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls0 <- matrix(NA, nrow = 9, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls0[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means0[ind, i],
                                               sd = sds0[ind, i]))
   mls0[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], weights = weights_TLP0, eval = FALSE))
   mls0[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_BLP0, eval = FALSE))
   mls0[6, j] <- mean(wrapper_SLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_SLP0, eval = FALSE))
   mls0[7, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_nBLP0, eval = FALSE))
   mls0[8, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means0[ind,], sds = sds0[ind,],
                                      pars = par_cBLP0,cab=comp_bpars0, eval = FALSE))
   mls0[9, j] <- mean(wrapper_cSLP_nonc(Y = Y[ind], means = means0[ind,], 
                                           sds = sds0[ind,], pars = par_cSLP0,ind_con = comp_c0, eval = FALSE))
 }

var0 <- matrix(NA, nrow = 9, ncol = 1,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                "Variance" ))
 for(j in 1){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls0[i, j]  <- PIT_normal(Y = Y[ind], mu = means0[ind, i],
                                               sd = sds0[ind, i],var=TRUE)
   var0[4, j] <- wrapper_TLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], weights = weights_TLP0, eval = FALSE, varPIT=TRUE)
   var0[5, j] <- wrapper_BLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_BLP0, eval = FALSE, varPIT=TRUE)
   var0[6, j] <- wrapper_SLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_SLP0, eval = FALSE, varPIT=TRUE)
   var0[7, j] <- wrapper_nBLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_nBLP0, eval = FALSE, varPIT=TRUE)
   var0[8, j] <- wrapper_cBLP_nonc(Y = Y[ind], means = means0[ind,], sds = sds0[ind,],
                                      pars = par_cBLP0,cab=comp_bpars0, eval = FALSE, varPIT=TRUE)
   var0[9, j] <- wrapper_cSLP_nonc(Y = Y[ind], means = means0[ind,], 
                                           sds = sds0[ind,], pars = par_cSLP0,ind_con = comp_c0, eval = FALSE, varPIT=TRUE)
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls2<-round(mls2, 3)
knitr::kable(list(table2,mls2), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
``` 

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means[!ind_est ,i], 
                         sd = sds[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], weights = weights_TLP2, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_BLP2, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], weights = weights_bcTLP2, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_bcBLP2, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_nBLP2, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_cBLP2, cab=comp_bpars2, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_cbcBLP2, 
            cab=cbcomp_bpars2, plot = TRUE)
```



## Scenario 3 : Higher variance forecast scenario

We modified the standard deviation of the first density forecast by adding a constant of 2 as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1,1+a^2_2+a^2_3+2)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
#------------ Scenario 2: some models are overconfident, others underconfident? (edited)---------- 
#
# Simulation setup ------------------------------------------------------------

# coefs2 <- c(a0 = 1, a1 = 1, a2 = -1, a3 = 1.1)
# means2 <- sds2 <- matrix(NA, nrow = length(Y), ncol = 3)
# 
# for(i in 1:ncol(means2)){
#   ind <- 1:4 %in% c(1, i+1)
#   means2[,i]  <- vars[, ind] %*% coefs2[ind]
#   sds2[,i]    <- sqrt(1 + sum(coefs2[!ind]^2)+3*(i==1))
# }

coefs2 <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
means2 <- sds2 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means2)){
  ind <- 1:4 %in% c(1, i+1)
  means2[,i]  <- vars[, ind] %*% coefs2[ind]
  sds2[,i]    <- sqrt(1 + sum(coefs2[!ind]^2)+2*(i==1))
}

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP3<-get_TLPpars(means2,sds2)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP3<-get_BLPpars(means2,sds2)
weights_BLP3<-par_BLP3[-c(1:2)]
ab3<-par_BLP3[c(1:2)]

# SLP (spread-adjustment linear pool) combination -----------------------------------
par_SLP3<-get_SLPpars(means2,sds2)
weights_SLP3<-par_SLP3[-1]
c3<-par_SLP3[1]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP3<-get_nBLPpars(means2,sds2)
weights_nBLP3<-par_nBLP3[-c(1:3)]
abc3<-par_nBLP3[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars3<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab3<-get_indBLPpars(means2,sds2,i)
  comp_bpars3[i,1]<-ab3[1]
  comp_bpars3[i,2]<-ab3[2]
}

# BLP components in BLP
par_cBLP3<-get_cBLPpars(means2,sds2,comp_bpars3)
weights_cBLP3<-par_cBLP3[-c(1:2)]
cBLP_ab3<-par_cBLP3[c(1:2)]

# component-specific SLP combination ------------------------------
comp_c3<-matrix(NA,ncol=1,nrow=3)
for(i in 1:3){
  ci3<-get_indSLPpars(means2,sds2,i)
  comp_c3[i]<-ci3[1]}
### SLP components in SLP
par_cSLP3<-get_cSLPpars(means2,sds2,comp_c3)
weights_cSLP3<-par_cSLP3[-1]
cSLP_c3<-par_cSLP3[1]
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table3 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table3[1,] <- c(weights_TLP3, rep(NA, 3))
table3[2,] <- c(par_BLP3[-c(1:2)], par_BLP3[1:2],NA)
table3[3,] <- c(weights_bcTLP3, rep(NA, 3))
table3[4,] <- c(par_bcBLP3[-c(1,2)], par_bcBLP3[1:2],NA)
table3[5,] <- c(par_nBLP3[-c(1:3)], par_nBLP3[1:3])
table3[6,] <- c(par_cBLP3[-c(1:2)], par_cBLP3[1:2],NA)
table3[7,] <- c(par_cbcBLP3[-c(1:2)], par_cbcBLP3[1:2],NA)
table3<-round(table3, 3)

# get biased corrected forecast for test set
bc_ests3<-bc_ests(Y, means2)
bc_forecasts3_all<-cbind(bc_ests3[1,1]+(bc_ests3[1,2]*means2[,1]),
                          bc_ests3[2,1]+(bc_ests3[2,2]*means2[,2]),
                          bc_ests3[3,1]+(bc_ests3[3,2]*means2[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls3 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls3[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means2[ind, i],
                                               sd = sds2[ind, i]))
   mls3[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], weights = weights_TLP3, eval = FALSE))
   mls3[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], pars = par_BLP3, eval = FALSE))
   mls3[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,],
                                   sds = sds2[ind,], weights = weights_bcTLP3, eval = FALSE))
   mls3[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,],
                                   sds = sds2[ind,], pars = par_bcBLP3, eval = FALSE))
   mls3[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], pars = par_nBLP3, eval = FALSE))
   mls3[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means2[ind,], sds = sds2[ind,],
                                      pars = par_cBLP3,cab=comp_bpars3, eval = FALSE))
   mls3[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,], 
                                           sds = sds2[ind,], pars = par_cbcBLP3,
                                           cab=cbcomp_bpars3, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls3<-round(mls3, 3)
knitr::kable(list(table3,mls3), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```  

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means2[!ind_est ,i], 
                         sd = sds2[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], weights = weights_TLP3, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_BLP3, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], weights = weights_bcTLP3, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_bcBLP3, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_nBLP3, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_cBLP3, cab=comp_bpars3, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_cbcBLP3, 
            cab=cbcomp_bpars3, plot = TRUE)
```


\clearpage

## Scenario 4 Biased + higher variance forecast scenario 

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, but we modified the standard deviation of the first and second density forecast as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1+N(2,1),1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3+2)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
# Scenario 3: some models are biased?
#
#
#
# coefs3 <- c(a0 = 1, a1 = 1, a2 = -1, a3 = -1.5)
# means3 <- sds3 <- matrix(NA, nrow = length(Y), ncol = 3)
# 
# for(i in 1:ncol(means3)){
#   ind <- 1:4 %in% c(1, i+1)
#   means3[,i]  <- vars[, ind] %*% coefs3[ind]
#   sds3[,i]    <- sqrt(1 + sum(coefs3[!ind]^2)+3*(i==1))
# }

coefs3 <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
means3 <- sds3 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means3)){
  ind <- 1:4 %in% c(1, i+1)
  means3[,i]  <- vars[, ind] %*% coefs3[ind]+noise*(i==1)
  sds3[,i]    <- sqrt(1 + sum(coefs3[!ind]^2)+2*(i==2))
}
# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP4<-get_TLPpars(means3,sds3)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP4<-get_BLPpars(means3,sds3)
weights_BLP4<-par_BLP4[-c(1:2)]
ab4<-par_BLP4[c(1:2)]

# SLP (spread-adjustment linear pool) combination -----------------------------------
par_SLP4<-get_SLPpars(means3,sds3)
weights_SLP4<-par_SLP4[-1]
c4<-par_SLP4[1]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP4<-get_nBLPpars(means3,sds3)
weights_nBLP4<-par_nBLP4[-c(1:3)]
abc4<-par_nBLP4[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars4<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab4<-get_indBLPpars(means3,sds3,i)
  comp_bpars4[i,1]<-ab4[1]
  comp_bpars4[i,2]<-ab4[2]
}

# BLP components in BLP
par_cBLP4<-get_cBLPpars(means3,sds3,comp_bpars4)
weights_cBLP4<-par_cBLP4[-c(1:2)]
cBLP_ab4<-par_cBLP4[c(1:2)]

# component-specific SLP combination ------------------------------
comp_c4<-matrix(NA,ncol=1,nrow=3)
for(i in 1:3){
  ci4<-get_indSLPpars(means3,sds3,i)
  comp_c4[i]<-ci4[1]}
### SLP components in SLP
par_cSLP3<-get_cSLPpars(means3,sds3,comp_c4)
weights_cSLP3<-par_cSLP3[-1]
cSLP_c3<-par_cSLP3[1]
```


```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table4 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table4[1,] <- c(weights_TLP4, rep(NA, 3))
table4[2,] <- c(par_BLP4[-c(1:2)], par_BLP4[1:2],NA)
table4[3,] <- c(weights_bcTLP4, rep(NA, 3))
table4[4,] <- c(par_bcBLP4[-c(1,2)], par_bcBLP4[1:2],NA)
table4[5,] <- c(par_nBLP4[-c(1:3)], par_nBLP4[1:3])
table4[6,] <- c(par_cBLP4[-c(1:2)], par_cBLP4[1:2],NA)
table4[7,] <- c(par_cbcBLP4[-c(1:2)], par_cbcBLP4[1:2],NA)
table4<-round(table4, 3)

# get biased corrected forecast for test set
bc_ests4<-bc_ests(Y, means3)
bc_forecasts4_all<-cbind(bc_ests4[1,1]+(bc_ests4[1,2]*means3[,1]),
                          bc_ests4[2,1]+(bc_ests4[2,2]*means3[,2]),
                          bc_ests4[3,1]+(bc_ests4[3,2]*means3[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls4 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls4[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means3[ind, i],
                                               sd = sds3[ind, i]))
   mls4[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], weights = weights_TLP4, eval = FALSE))
   mls4[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], pars = par_BLP4, eval = FALSE))
   mls4[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,],
                                   sds = sds3[ind,], weights = weights_bcTLP4, eval = FALSE))
   mls4[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,],
                                   sds = sds3[ind,], pars = par_bcBLP4, eval = FALSE))
   mls4[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], pars = par_nBLP4, eval = FALSE))
   mls4[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means3[ind,], sds = sds3[ind,],
                                      pars = par_cBLP4,cab=comp_bpars4, eval = FALSE))
   mls4[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,], 
                                           sds = sds3[ind,], pars = par_cbcBLP4,
                                           cab=cbcomp_bpars4, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls4<-round(mls4, 3)
knitr::kable(list(table4,mls4), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```   

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means3[!ind_est ,i], 
                         sd = sds3[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], weights = weights_TLP4, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_BLP4, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], weights = weights_bcTLP4, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_bcBLP4, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_nBLP4, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_cBLP4, cab=comp_bpars4, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_cbcBLP4, 
            cab=cbcomp_bpars4, plot = TRUE)
```


