---
title: "BLP Simulation"
author: "Nutcha Wattanachit"
date: "12/2/2019"
header-includes:
   - \usepackage{booktabs}
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{multicol}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage{makecell}
   - \usepackage{xcolor}
output:
  pdf_document:
        keep_tex: true
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library("pipeR");library("dplyr");library("cdcfluview")
library(fGarch);library(Matrix)
library(quantreg);library(tidyverse);library(cowplot)
library(data.table);library(reshape2);library(gridExtra)
library(rmutil);library(ranger)
library(xgboost);library(splines);library(ggplot2)
library(xtable);library(here)
library(stats);library(mixtools);library(cowplot)
library(ggforce);library(grid);library(FluSight)
library(rmutil);library(here);library(stats)
library(mixtools);library(knitr);library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, comment = FALSE, message=FALSE, fig.show= 'hold',
                      table.placement='H')
options(kableExtra.latex.load_packages = FALSE)
```

# Ensemble/pooling Methods

## TLP

Traditional linear pool finds optimal weights that maximmizes the likelihood of $f(y)=\sum_{i=1}^k w_if_i(y)$.


## BLP

BLP finds $\alpha$, $\beta$, and weights that maximize the likelihood of 

$$g_{\alpha,\beta}=(\sum_{i=1}^k w_if_i(y))b_{\alpha,\beta}(\sum_{i=1}^k w_iF_i(y)).$$

 BLP Example: To obtain $\alpha$, $\beta$, and the weights for all component models, train the BLP model on half of the data. Then, use $\alpha$, $\beta$, and the weights from training to apply to the data held out for testing. 

## Bias-corrected TLP (bcTLP)

This method corrects for bias of the component models, and then use the traditional method to generate the ensemble. The difference between this and the traditional TLP is that the component models inputted in bcTLP are bias-corrected. The bias correction method used is simple linear regression (Raftery, 2005). By regressing $y$ against the forecast output, we obtain the intercept and coefficient of each component, 

$$g_i(y)=\alpha_i+\beta_if_i(y)$$.

and the final ensemble is 

$$g(y)=\sum_{i=1}^k w_ig_i(y)$$.

## Bias-corrected BLP (bcBLP)

This method also corrects for bias of the component models using linear regression, and then use the BLP method to generate the ensemble. The final ensemble is 

$$g_{\alpha,\beta}=(\sum_{i=1}^k w_ig_i)b_{\alpha,\beta}(\sum_{i=1}^k w_iG_i(y)).$$


## BLP with Non-central Parameter(nBLP)

nBLP finds $\alpha$, $\beta$, non-central parameter $\lambda$, and weights that maximize the likelihood of 

$g_{\alpha,\beta,\lambda}=(\sum_{i=1}^k w_if_i(y))b_{\alpha,\beta}(\sum_{i=1}^k w_iF_i(y)).$

nBLP process:  To obtain $\alpha$, $\beta$, $\lambda$, and the weights for all component models, train the nBLP model on half of the data. Then, use $\alpha$, $\beta$, $\lambda$, and the weights from training to apply to the data held out for testing.
 
## Component-wise BLP (cBLP)

This is the extension of the traditional BLP. We beta-transform each of the cumulative distribution functions of the component models. This is done by finding $\alpha$ and $\beta$ that maximize the likelihood of 

$$
\begin{aligned}
G_{i,\alpha_i,\beta_i} &= B_{\alpha_i,\beta_i}[F_i(y)]\\
g_{i,\alpha_i,\beta_i} &= f_i(y) \times b_{\alpha_i,\beta_i}[F_i(y)]
\end{aligned}
$$

Then, to obtain $\alpha$, $\beta$, and the weights for 21 models, we apply BLP on the beta-transformed components: 

$$
\begin{aligned}
G_{\alpha,\beta} &= B_{\alpha,\beta}\Big[\sum_{i=1}^k w_iB_{\alpha_i,\beta_i}(F_i(y))\Big]\\
g_{\alpha,\beta} &= \Big(\sum_{i=1}^k w_ib_{i,\alpha_i,\beta_i}(F_i(y))f_i(y)  \Big)b_{\alpha,\beta} \Big(\sum_{i=1}^k w_iB_{i,\alpha_i,\beta_i}(F_i(y))\Big)
\end{aligned}
$$

cBLP - Part 1: For each component model, train over all observations to get $\alpha_i$ and $\beta_i$. Then, apply $\alpha_i$ and $\beta_i$ to beta-transform the CDF. This ends the component-wise part. 

cBLP Part 2: Apply the usual BLP process on the beta-transformed component models to get the BLP ensemble. 

## Componentwise Bias-Corrected & Componentwise Recalibrated BLP (cbcBLP) 

This method corrects for bias of the component models and also recalibrate them using beta transform. Then the BLP method is used to generate the ensemble. 

\clearpage

# Simulation studies

The data generating process for the observation $Y$ in the regression model is 

$$
Y=X_0+a_1X_1+a_2X_2+a_3X_3+\epsilon, \epsilon\sim(0,1)
$$

where $a_1,a_2,$ and $a_3$ are real constants that vary across different simulation studies, and $X_0,X_1,X_2,X_3,$ and $\epsilon$ are independent, standard normal random variables. The individual predictive densities have partial access of the above set of covariates. $f_1$ has access to only $X_0$ and $X_1$, $f_2$ has access to only $X_0$ and $X_2$, and $f_3$ has access to only $X_0$ and $X_3$. We want to combine $f_1,f_2,$ and $f_3$ to predict $Y$. In this setup, $X_0$ represent shared information, while other covariates represent information unique to each individual model.

We estimate the pooling/combination formulas on a random sample ${(f_{1i} , f_{2i} , f_{3i}, Y_i) : i = 1,..., n}$ of size $n = 50,000$ and evaluate on an independent test sample of the same size.

## Scenario 1: calibrated components (Baseline scenario).

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, so that $f_3$ is a more concentrated, sharper density forecast than $f_1$ and $f_2$ (Gneiting and Ranjan (2013)) and they are defined as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1,1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r, include=FALSE,cache=TRUE}
source("./new_simBLP.R")
# -----------Scenario 0: Simulation setup -----------------------#
n <- 1e5

coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))

Y       <- vars %*% coefs + rnorm(n = n)

means0 <- sds0 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means0)){
  ind <- 1:4 %in% c(1, i+1)
  means0[,i]  <- vars[, ind] %*% coefs[ind]
  sds0[,i]    <- sqrt(1 + sum(coefs[!ind]^2))
}

forecast_names <- c("TLP", "BLP", "bcTLP","bcBLP","nBLP", "cBLP","cbcBLP")

ind_est <- c(rep(TRUE, n/2), rep(FALSE, n/2))

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP0<-get_TLPpars(means0,sds0)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP0<-get_BLPpars(means0,sds0)
weights_BLP0<-par_BLP0[-c(1:2)]
ab0<-par_BLP0[c(1:2)]

# bcTLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
bc_forecasts0<-bc_slr(Y,means0)
weights_bcTLP0<-get_bcTLPpars(bc_forecasts0,sds0)
# bcBLP (Beta-transformed linear pool) combination ------------------------------
par_bcBLP0<-get_bcBLPpars(bc_forecasts0,sds0)
weights_bcBLP0<-par_bcBLP0[-c(1:2)]
bcBLP_ab0<-par_bcBLP0[c(1:2)]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP0<-get_nBLPpars(means0,sds0)
weights_nBLP0<-par_nBLP0[-c(1:3)]
abc0<-par_nBLP0[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars0<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab0<-get_indBLPpars(means0,sds0,i)
  comp_bpars0[i,1]<-ab0[1]
  comp_bpars0[i,2]<-ab0[2]
}

### BLP components in BLP
par_cBLP0<-get_cBLPpars(means0,sds0,comp_bpars0)
weights_cBLP0<-par_cBLP0[-c(1:2)]
cBLP_ab0<-par_cBLP0[c(1:2)]

# cbcBLP (biased-correct & recalibrate each comp) ------------------------------
cbcomp_bpars0<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  cbc_ab0<-get_ind_cbcBLPpars(bc_forecasts0,sds0,i)
  cbcomp_bpars0[i,1]<-cbc_ab0[1]
  cbcomp_bpars0[i,2]<-cbc_ab0[2]
}
### BLP components in cbcBLP
par_cbcBLP0<-get_cbcBLPpars(bc_forecasts0,sds0,cbcomp_bpars0)
weights_cbcBLP0<-par_cbcBLP0[-c(1:2)]
cbcBLP_ab0<-par_cbcBLP0[c(1:2)]
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table0 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table0[1,] <- c(weights_TLP0, rep(NA, 3))
table0[2,] <- c(par_BLP0[-c(1:2)], par_BLP0[1:2],NA)
table0[3,] <- c(weights_bcTLP0, rep(NA, 3))
table0[4,] <- c(par_bcBLP0[-c(1,2)], par_bcBLP0[1:2],NA)
table0[5,] <- c(par_nBLP0[-c(1:3)], par_nBLP0[1:3])
table0[6,] <- c(par_cBLP0[-c(1:2)], par_cBLP0[1:2],NA)
table0[7,] <- c(par_cbcBLP0[-c(1:2)], par_cbcBLP0[1:2],NA)
table0<-round(table0, 3)

# get biased corrected forecast for test set
bc_ests0<-bc_ests(Y, means0)
bc_forecasts0_all<-cbind(bc_ests0[1,1]+(bc_ests0[1,2]*means0[,1]),
                          bc_ests0[2,1]+(bc_ests0[2,2]*means0[,2]),
                          bc_ests0[3,1]+(bc_ests0[3,2]*means0[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls0 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls0[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means0[ind, i],
                                               sd = sds0[ind, i]))
   mls0[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], weights = weights_TLP0, eval = FALSE))
   mls0[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_BLP0, eval = FALSE))
   mls0[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts0_all[ind,],
                                   sds = sds0[ind,], weights = weights_bcTLP0, eval = FALSE))
   mls0[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts0_all[ind,],
                                   sds = sds0[ind,], pars = par_bcBLP0, eval = FALSE))
   mls0[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means0[ind,],
                                   sds = sds0[ind,], pars = par_nBLP0, eval = FALSE))
   mls0[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means0[ind,], sds = sds0[ind,],
                                      pars = par_cBLP0,cab=comp_bpars0, eval = FALSE))
   mls0[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts0_all[ind,], 
                                           sds = sds0[ind,], pars = par_cbcBLP0,
                                           cab=cbcomp_bpars0, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
# Overview over estimated parameters
mls0<-round(mls0, 3)
knitr::kable(list(table0,mls0), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```


```{r,fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
# PIT Histogramme
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means0[!ind_est ,i], 
                         sd = sds0[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], weights = weights_TLP0, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_BLP0, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts0_all[!ind_est, ],
            sds = sds0[!ind_est, ], weights = weights_bcTLP0, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts0_all[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_bcBLP0, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_nBLP0, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means0[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_cBLP0, cab=comp_bpars0, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts0_all[!ind_est, ],
            sds = sds0[!ind_est, ], pars = par_cbcBLP0, 
            cab=cbcomp_bpars0, plot = TRUE)
#dev.off()
```



\clearpage

## Scenario 2: Biased forecast scenario

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, and we add $N(2,1)$ to the mean of $f_1$ so that it is a biased forecast. The models are defined as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1+N(2,1),1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
#------------scene 1 Simulation setup ------------------------------------------------------------

# coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
# vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))
# 
# Y       <- vars %*% coefs + rnorm(n = n)
# means <- sds <- matrix(NA, nrow = length(Y), ncol = 3)
# 
# for(i in 1:ncol(means)){
#   ind <- 1:4 %in% c(1, i+1)
#   means[,i]  <- vars[, ind] %*% coefs[ind]
#   sds[,i]    <- sqrt(1 + sum(coefs[!ind]^2)+3*(i==1)+5*(i==2)+10*(i==3))
# }

coefs <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
vars  <- matrix(rnorm(n = n*4), nrow = n, dimnames = list(NULL, c("x0", "x1", "x2", "x3")))
noise<-rnorm(n,2,1)

Y       <- vars %*% coefs + rnorm(n = n)
means <- sds <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means)){
  ind <- 1:4 %in% c(1, i+1)
  means[,i]  <- vars[, ind] %*% coefs[ind]+noise*(i==1)
  sds[,i]    <- sqrt(1 + sum(coefs[!ind]^2))
}

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP2<-get_TLPpars(means,sds)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP2<-get_BLPpars(means,sds)
weights_BLP2<-par_BLP2[-c(1:2)]
ab2<-par_BLP2[c(1:2)]

# bcTLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
bc_forecasts2<-bc_slr(Y,means)
weights_bcTLP2<-get_bcTLPpars(bc_forecasts2,sds)
# bcBLP (Beta-transformed linear pool) combination ------------------------------
par_bcBLP2<-get_bcBLPpars(bc_forecasts2,sds)
weights_bcBLP2<-par_bcBLP2[-c(1:2)]
bcBLP_ab2<-par_bcBLP2[c(1:2)]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP2<-get_nBLPpars(means,sds)
weights_nBLP2<-par_nBLP2[-c(1:3)]
abc2<-par_nBLP2[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars2<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab2<-get_indBLPpars(means,sds,i)
  comp_bpars2[i,1]<-ab2[1]
  comp_bpars2[i,2]<-ab2[2]
}

# BLP components in BLP
par_cBLP2<-get_cBLPpars(means,sds,comp_bpars2)
weights_cBLP2<-par_cBLP2[-c(1:2)]
cBLP_ab2<-par_cBLP2[c(1:2)]

# cbcBLP (biased-correct & recalibrate each comp) ------------------------------
cbcomp_bpars2<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  cbc_ab2<-get_ind_cbcBLPpars(bc_forecasts2,sds,i)
  cbcomp_bpars2[i,1]<-cbc_ab2[1]
  cbcomp_bpars2[i,2]<-cbc_ab2[2]
}
### BLP components in cbcBLP
par_cbcBLP2<-get_cbcBLPpars(bc_forecasts2,sds,cbcomp_bpars2)
weights_cbcBLP2<-par_cbcBLP2[-c(1:2)]
cbcBLP_ab2<-par_cbcBLP2[c(1:2)]
```

```{r,out.width='60%',out.height='60%',fig.align='center'}
com<-data.frame(cbind(Y[ind],means[ind,1],bc_forecasts2[1]))
names(com)<-c("Y","of1","g1")
com$f1<-com$Y-com$of1
com$bc_f1<-com$Y-com$g1
biasdat<-melt(com[,4:5])
names(biasdat)[1]<-"Difference"
ggplot(biasdat,aes(x = value, fill = Difference))+
  geom_density(alpha=0.5)+
  labs(title="Distribution of Differrences between Y and forecast means")
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table2 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table2[1,] <- c(weights_TLP2, rep(NA, 3))
table2[2,] <- c(par_BLP2[-c(1:2)], par_BLP2[1:2],NA)
table2[3,] <- c(weights_bcTLP2, rep(NA, 3))
table2[4,] <- c(par_bcBLP2[-c(1,2)], par_bcBLP2[1:2],NA)
table2[5,] <- c(par_nBLP2[-c(1:3)], par_nBLP2[1:3])
table2[6,] <- c(par_cBLP2[-c(1:2)], par_cBLP2[1:2],NA)
table2[7,] <- c(par_cbcBLP2[-c(1:2)], par_cbcBLP2[1:2],NA)

table2<-round(table2, 3)

# get biased corrected forecast for test set
# use the coefficients from the train set so it's a+means*beta
bc_ests2<-bc_ests(Y, means)
bc_forecasts2_all<-cbind(bc_ests2[1,1]+(bc_ests2[1,2]*means[,1]),
                          bc_ests2[2,1]+(bc_ests2[2,2]*means[,2]),
                          bc_ests2[3,1]+(bc_ests2[3,2]*means[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls2 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls2[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means[ind, i],
                                               sd = sds[ind, i]))
   mls2[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], weights = weights_TLP2, eval = FALSE))
   mls2[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_BLP2, eval = FALSE))
   mls2[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], weights = weights_bcTLP2, eval = FALSE))
   mls2[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,],
                                   sds = sds[ind,], pars = par_bcBLP2, eval = FALSE))
   mls2[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means[ind,],
                                   sds = sds[ind,], pars = par_nBLP2, eval = FALSE))
   mls2[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means[ind,], sds = sds[ind,],
                                      pars = par_cBLP2,cab=comp_bpars2, eval = FALSE))
   mls2[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts2_all[ind,], 
                                           sds = sds[ind,], pars = par_cbcBLP2,
                                           cab=cbcomp_bpars2, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls2<-round(mls2, 3)
knitr::kable(list(table2,mls2), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
``` 

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means[!ind_est ,i], 
                         sd = sds[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], weights = weights_TLP2, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_BLP2, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], weights = weights_bcTLP2, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_bcBLP2, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_nBLP2, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_cBLP2, cab=comp_bpars2, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts2_all[!ind_est, ],
            sds = sds[!ind_est, ], pars = par_cbcBLP2, 
            cab=cbcomp_bpars2, plot = TRUE)
```



## Scenario 3 : Higher variance forecast scenario

We modified the standard deviation of the first density forecast by adding a constant of 2 as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1,1+a^2_2+a^2_3+2)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
#------------ Scenario 2: some models are overconfident, others underconfident? (edited)---------- 
#
# Simulation setup ------------------------------------------------------------

# coefs2 <- c(a0 = 1, a1 = 1, a2 = -1, a3 = 1.1)
# means2 <- sds2 <- matrix(NA, nrow = length(Y), ncol = 3)
# 
# for(i in 1:ncol(means2)){
#   ind <- 1:4 %in% c(1, i+1)
#   means2[,i]  <- vars[, ind] %*% coefs2[ind]
#   sds2[,i]    <- sqrt(1 + sum(coefs2[!ind]^2)+3*(i==1))
# }

coefs2 <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
means2 <- sds2 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means2)){
  ind <- 1:4 %in% c(1, i+1)
  means2[,i]  <- vars[, ind] %*% coefs2[ind]
  sds2[,i]    <- sqrt(1 + sum(coefs2[!ind]^2)+2*(i==1))
}

# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP3<-get_TLPpars(means2,sds2)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP3<-get_BLPpars(means2,sds2)
weights_BLP3<-par_BLP3[-c(1:2)]
ab3<-par_BLP3[c(1:2)]

# bcTLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
bc_forecasts3<-bc_slr(Y,means2)
weights_bcTLP3<-get_bcTLPpars(bc_forecasts3,sds2)
# bcBLP (Beta-transformed linear pool) combination ------------------------------
par_bcBLP3<-get_bcBLPpars(bc_forecasts3,sds2)
weights_bcBLP3<-par_bcBLP3[-c(1:2)]
bcBLP_ab3<-par_bcBLP3[c(1:2)]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP3<-get_nBLPpars(means2,sds2)
weights_nBLP3<-par_nBLP3[-c(1:3)]
abc3<-par_nBLP3[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars3<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab3<-get_indBLPpars(means2,sds2,i)
  comp_bpars3[i,1]<-ab3[1]
  comp_bpars3[i,2]<-ab3[2]
}

# BLP components in BLP
par_cBLP3<-get_cBLPpars(means2,sds2,comp_bpars3)
weights_cBLP3<-par_cBLP3[-c(1:2)]
cBLP_ab3<-par_cBLP3[c(1:2)]

# cbcBLP (biased-correct & recalibrate each comp) ------------------------------
cbcomp_bpars3<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  cbc_ab3<-get_ind_cbcBLPpars(bc_forecasts3,sds2,i)
  cbcomp_bpars3[i,1]<-cbc_ab3[1]
  cbcomp_bpars3[i,2]<-cbc_ab3[2]
}
### BLP components in cbcBLP
par_cbcBLP3<-get_cbcBLPpars(bc_forecasts3,sds2,cbcomp_bpars3)
weights_cbcBLP3<-par_cbcBLP3[-c(1:2)]
cbcBLP_ab3<-par_cbcBLP3[c(1:2)]
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table3 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table3[1,] <- c(weights_TLP3, rep(NA, 3))
table3[2,] <- c(par_BLP3[-c(1:2)], par_BLP3[1:2],NA)
table3[3,] <- c(weights_bcTLP3, rep(NA, 3))
table3[4,] <- c(par_bcBLP3[-c(1,2)], par_bcBLP3[1:2],NA)
table3[5,] <- c(par_nBLP3[-c(1:3)], par_nBLP3[1:3])
table3[6,] <- c(par_cBLP3[-c(1:2)], par_cBLP3[1:2],NA)
table3[7,] <- c(par_cbcBLP3[-c(1:2)], par_cbcBLP3[1:2],NA)
table3<-round(table3, 3)

# get biased corrected forecast for test set
bc_ests3<-bc_ests(Y, means2)
bc_forecasts3_all<-cbind(bc_ests3[1,1]+(bc_ests3[1,2]*means2[,1]),
                          bc_ests3[2,1]+(bc_ests3[2,2]*means2[,2]),
                          bc_ests3[3,1]+(bc_ests3[3,2]*means2[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls3 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls3[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means2[ind, i],
                                               sd = sds2[ind, i]))
   mls3[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], weights = weights_TLP3, eval = FALSE))
   mls3[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], pars = par_BLP3, eval = FALSE))
   mls3[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,],
                                   sds = sds2[ind,], weights = weights_bcTLP3, eval = FALSE))
   mls3[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,],
                                   sds = sds2[ind,], pars = par_bcBLP3, eval = FALSE))
   mls3[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means2[ind,],
                                   sds = sds2[ind,], pars = par_nBLP3, eval = FALSE))
   mls3[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means2[ind,], sds = sds2[ind,],
                                      pars = par_cBLP3,cab=comp_bpars3, eval = FALSE))
   mls3[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts3_all[ind,], 
                                           sds = sds2[ind,], pars = par_cbcBLP3,
                                           cab=cbcomp_bpars3, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls3<-round(mls3, 3)
knitr::kable(list(table3,mls3), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```  

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means2[!ind_est ,i], 
                         sd = sds2[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], weights = weights_TLP3, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_BLP3, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], weights = weights_bcTLP3, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_bcBLP3, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_nBLP3, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means2[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_cBLP3, cab=comp_bpars3, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts3_all[!ind_est, ],
            sds = sds2[!ind_est, ], pars = par_cbcBLP3, 
            cab=cbcomp_bpars3, plot = TRUE)
```


\clearpage

## Scenario 4 Biased + higher variance forecast scenario 

In this scenario, $a_1 = a_2 = 1$ and $a_3 = 1.1$, but we modified the standard deviation of the first and second density forecast as follows:

$$
\begin{aligned}
f_1&=\text{N}(X_0+a_1X_1+N(2,1),1+a^2_2+a^2_3)\\
f_2&=\text{N}(X_0+a_2X_2,1+a^2_1+a^2_3+2)\\
f_3&=\text{N}(X_0+a_3X_3,1+a^2_1+a^2_2)\\
\end{aligned}
$$

```{r,include=FALSE,cache=TRUE}
# Scenario 3: some models are biased?
#
#
#
# coefs3 <- c(a0 = 1, a1 = 1, a2 = -1, a3 = -1.5)
# means3 <- sds3 <- matrix(NA, nrow = length(Y), ncol = 3)
# 
# for(i in 1:ncol(means3)){
#   ind <- 1:4 %in% c(1, i+1)
#   means3[,i]  <- vars[, ind] %*% coefs3[ind]
#   sds3[,i]    <- sqrt(1 + sum(coefs3[!ind]^2)+3*(i==1))
# }

coefs3 <- c(a0 = 1, a1 = 1, a2 = 1, a3 = 1.1)
means3 <- sds3 <- matrix(NA, nrow = length(Y), ncol = 3)

for(i in 1:ncol(means3)){
  ind <- 1:4 %in% c(1, i+1)
  means3[,i]  <- vars[, ind] %*% coefs3[ind]+noise*(i==1)
  sds3[,i]    <- sqrt(1 + sum(coefs3[!ind]^2)+2*(i==2))
}
# TLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
weights_TLP4<-get_TLPpars(means3,sds3)

# BLP (Beta-transformed linear pool) combination ------------------------------
par_BLP4<-get_BLPpars(means3,sds3)
weights_BLP4<-par_BLP4[-c(1:2)]
ab4<-par_BLP4[c(1:2)]

# bcTLP (Traditional linear pool) combination -----------------------------------
# Simple linear combination of forecasts
bc_forecasts4<-bc_slr(Y,means3)
weights_bcTLP4<-get_bcTLPpars(bc_forecasts4,sds3)
# bcBLP (Beta-transformed linear pool) combination ------------------------------
par_bcBLP4<-get_bcBLPpars(bc_forecasts4,sds3)
weights_bcBLP4<-par_bcBLP4[-c(1:2)]
bcBLP_ab4<-par_bcBLP4[c(1:2)]

# NBLP (Beta-transformed linear pool) combination ------------------------------
par_nBLP4<-get_nBLPpars(means3,sds3)
weights_nBLP4<-par_nBLP4[-c(1:3)]
abc4<-par_nBLP4[c(1:3)]

# component-specific BLP (Beta-transformed linear pool) combination ------------------------------
comp_bpars4<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  ab4<-get_indBLPpars(means3,sds3,i)
  comp_bpars4[i,1]<-ab4[1]
  comp_bpars4[i,2]<-ab4[2]
}

# BLP components in BLP
par_cBLP4<-get_cBLPpars(means3,sds3,comp_bpars4)
weights_cBLP4<-par_cBLP4[-c(1:2)]
cBLP_ab4<-par_cBLP4[c(1:2)]

# cbcBLP (biased-correct & recalibrate each comp) ------------------------------
cbcomp_bpars4<-matrix(NA,ncol=2,nrow=3)
for(i in 1:3){
  cbc_ab4<-get_ind_cbcBLPpars(bc_forecasts4,sds3,i)
  cbcomp_bpars4[i,1]<-cbc_ab4[1]
  cbcomp_bpars4[i,2]<-cbc_ab4[2]
}
### BLP components in cbcBLP
par_cbcBLP4<-get_cbcBLPpars(bc_forecasts4,sds3,cbcomp_bpars4)
weights_cbcBLP4<-par_cbcBLP4[-c(1:2)]
cbcBLP_ab4<-par_cbcBLP4[c(1:2)]
```

```{r,out.width='60%',out.height='60%',fig.align='center'}
com4<-data.frame(cbind(Y[ind],means3[ind,1],bc_forecasts4[1]))
names(com4)<-c("Y","of1","g1")
com4$f1<-com4$Y-com4$of1
com4$bc_f1<-com4$Y-com4$g1
biasdat4<-melt(com4[,4:5])
names(biasdat4)[1]<-"Difference"
ggplot(biasdat4,aes(x = value, fill = Difference))+
  geom_density(alpha=0.5)+
  labs(title="Distribution of Differrences between Y and forecast means")
```

```{r, include=FALSE}
# Evaluation ------------------------------------------------------------------

# Parameter values
table4 <- matrix(NA, nrow = 7, ncol = 6, 
                dimnames = list(c(forecast_names), c("w1", "w2", "w3", "alpha", "beta","ncp")))
table4[1,] <- c(weights_TLP4, rep(NA, 3))
table4[2,] <- c(par_BLP4[-c(1:2)], par_BLP4[1:2],NA)
table4[3,] <- c(weights_bcTLP4, rep(NA, 3))
table4[4,] <- c(par_bcBLP4[-c(1,2)], par_bcBLP4[1:2],NA)
table4[5,] <- c(par_nBLP4[-c(1:3)], par_nBLP4[1:3])
table4[6,] <- c(par_cBLP4[-c(1:2)], par_cBLP4[1:2],NA)
table4[7,] <- c(par_cbcBLP4[-c(1:2)], par_cbcBLP4[1:2],NA)
table4<-round(table4, 3)

# get biased corrected forecast for test set
bc_ests4<-bc_ests(Y, means3)
bc_forecasts4_all<-cbind(bc_ests4[1,1]+(bc_ests4[1,2]*means3[,1]),
                          bc_ests4[2,1]+(bc_ests4[2,2]*means3[,2]),
                          bc_ests4[3,1]+(bc_ests4[3,2]*means3[,3]))

# # Mean log score
# # # LS(f,y) = -log(f(y))
mls4 <- matrix(NA, nrow = 10, ncol = 2,
                dimnames = list(c(c("f1","f2","f3"),forecast_names),
                                c("Training", "Test")))
 for(j in 1:2){
   ind <- if(j == 1){ind_est}else{!ind_est}
   for(i in 1:3) mls4[i, j]  <- mean(ll_normal(Y = Y[ind], mu = means3[ind, i],
                                               sd = sds3[ind, i]))
   mls4[4, j] <- mean(wrapper_TLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], weights = weights_TLP4, eval = FALSE))
   mls4[5, j] <- mean(wrapper_BLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], pars = par_BLP4, eval = FALSE))
   mls4[6, j] <- mean(wrapper_bcTLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,],
                                   sds = sds3[ind,], weights = weights_bcTLP4, eval = FALSE))
   mls4[7, j] <- mean(wrapper_bcBLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,],
                                   sds = sds3[ind,], pars = par_bcBLP4, eval = FALSE))
   mls4[8, j] <- mean(wrapper_nBLP_nonc(Y = Y[ind], means = means3[ind,],
                                   sds = sds3[ind,], pars = par_nBLP4, eval = FALSE))
   mls4[9, j] <- mean(wrapper_cBLP_nonc(Y = Y[ind], means = means3[ind,], sds = sds3[ind,],
                                      pars = par_cBLP4,cab=comp_bpars4, eval = FALSE))
   mls4[10, j] <- mean(wrapper_cbcBLP_nonc(Y = Y[ind], means = bc_forecasts4_all[ind,], 
                                           sds = sds3[ind,], pars = par_cbcBLP4,
                                           cab=cbcomp_bpars4, eval = FALSE))
}
```

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap=""}
mls4<-round(mls4, 3)
knitr::kable(list(table4,mls4), "latex", booktabs = T,caption = "Model Parameters and Log Score") %>%
   kable_styling(latex_options = c("striped", "hold_position"))
```   

```{r, fig.align="center",fig.pos='H',message=FALSE,warning=FALSE,fig.cap="",results='hide'}
par(mfrow = c(2,3))
for(i in 1:3) PIT_normal(Y = Y[!ind_est], mu = means3[!ind_est ,i], 
                         sd = sds3[!ind_est ,i], plot = TRUE, name = i)
wrapper_TLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], weights = weights_TLP4, plot = TRUE)
wrapper_BLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_BLP4, plot = TRUE)
wrapper_bcTLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], weights = weights_bcTLP4, plot = TRUE)
wrapper_bcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_bcBLP4, plot = TRUE)
wrapper_nBLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_nBLP4, plot = TRUE)
wrapper_cBLP_nonc(Y = Y[!ind_est], means = means3[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_cBLP4, cab=comp_bpars4, plot = TRUE)
wrapper_cbcBLP_nonc(Y = Y[!ind_est], means = bc_forecasts4_all[!ind_est, ],
            sds = sds3[!ind_est, ], pars = par_cbcBLP4, 
            cab=cbcomp_bpars4, plot = TRUE)
```


\clearpage
<!-- 

# BLP, Component-wise BLP (cBLP), and non-central BLP in the application - To be updated!!!


Introduced by Ranjan and Gneiting (2010), the BLP combination formula maps the component models $F_1,...,F_k$ to $G_{\alpha,\beta}=B_{\alpha,\beta}(\sum_{i=1}^k w_iF_i(y))$. The weights are nonnegative and sum to 1. The procedure in this project is as follows

## BLP, cBLP, and nBLP cross-validation - To be updated!!!

\begin{itemize}

\item The 2017/2018 season is excluded and will later be used as a prospective test season. The BLP and cBLP cross-validation process include season 2010/2011-2016/2017. Using leave-one-season-out cross-validation, each of the 2010/2011-2016/2017 seasons takes turn to be the test season with the other 6 seasons being the training seasons. So, we have 7 test seasons in the cross-validation process.

\item BLP Example: To obtain $\alpha$, $\beta$, and the weights for 21 component models with the 2010/2011 as a test season, I trained the BLP model on season 2011/2012-2016/2017. Then, for test season evaluation, I applied $\alpha$, $\beta$, and the weights I got from training in test season. Repeat this for all 7 seasons (woohoo!).

\item cBLP Example Part 1: Set the 2010/2011 as a test season. To apply the beta transformation on each of the component models, I optmized $\alpha$, $\beta$ that maximize the beta log-likelihood for each component model in the training seasons (2011/2012-2016/2017). Once I got all 21 pairs of $\alpha$ and $\beta$, I used them to beta-transform component models in the training seasons (2011/2012-2016/2017). This ends the component-wise part. 

\item cBLP Example Part 2: Now that I have 21 beta-transformed components, I can apply the usual BLP process on them. To obtain $\alpha$, $\beta$, and the 21 weights for the whole BLP ensemble, I trained BLP model on the training seasons (2011/2012-2016/2017). Then, for test season evaluation, I applied $\alpha$, $\beta$, and the weights I got from training in test season. Repeat this for all 7 seasons (woohoo again!).

\end{itemize}
 -->