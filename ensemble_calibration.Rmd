---
title: "Application results"
author: "Nutcha Wattanachit"
date: "06/29/2021"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{multicol}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{booktabs}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage{makecell}
   - \usepackage{xcolor}
output:
  pdf_document:
        keep_tex: true
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE,fig.pos = "H")
```

We compare 1) the equally-weighted ensemble (EW), 2) the traditional linear pool (TLP), 3) the beta-transform linear pool (BLP), 4) the equally-weighted beta-transform linear pool, 5) the finite beta mixture 6) the finite beta mixture with equally-weighted component forecasts in the simulation studies and in the application of influenza forecasting. For both beta mixture approaches, the number of mixing beta components are $K=2,3,4,5$. 

# Methods

Let $f_1,...,f_M$ be predictive density forecasts from $M$ component forecasting models, the ensemble methods combine the component forecasting models as follows

## Equally-weighted ensemble (EW)

The equally-weighted ensemble combines the component forecasting models with the aggregation predictive distribution function

\begin{align}
f_{\text{EW}}(y)=\sum_{m=1}^M \frac{1}{M}f_m(y).
\end{align}

 
## Traditional linear pool (TLP)

The TLP finds a set of optimal nonnegative weights $w_i$ that maximize the likelihood of the aggregation predictive distribution function

\begin{align}
f_{\text{TLP}}(y)=\sum_{m=1}^M w_mf_m(y),
\end{align}

where $\sum_{m=1}^M w_m=1$. The TLP is underdispersed when the component models are probabilistically calibrated.

## Beta-transform linear pool (BLP)

The BLP applies a beta transform on the combined predictive cumulative distribution function 

\begin{align}
F_{\text{BLP}}(y)=B_{\alpha,\beta}\Big(\sum_{m=1}^M w_m F_m(y)\Big),
\end{align}

Specifically, the BLP finds the transformation parameters $\alpha,\beta > 0$, and a set of nonnegative weights $w_m$ that maximize the likelihood of the aggregated predictive distribution function

\begin{align}
f_{\text{BLP}}(y)=\Big(\sum_{m=1}^M w_mf_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M w_m F_m(y)\Big),
\end{align}

where $b_{\alpha,\beta}$ denotes the beta density and $\sum_{m=1}^M w_m=1$.

## Equally-weighted beta-transform linear pool (EW-BLP)

The EW-BLP applies a beta transform on the equally-weighted ensemble and has the predictive cumulative distribution function

\begin{align}
F_{\text{EW-BLP}}(y)=B_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big),
\end{align}

The EW-BLP finds the transformation parameters $\alpha,\beta > 0$ that maximize the likelihood of the aggregated predictive distribution function

\begin{align}
f_{\text{EW-BLP}}(y)=\Big(\sum_{m=1}^M w_mf_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big).
\end{align}

## Finite beta mixture ($\text{BM}_k$)

The $\text{BM}_k$ extends the BLP method by using a finite beta mixture combination formula

\begin{align}
F_{\text{BM}_k}(y)=\sum_{k=1}^K w_kB_{\alpha,\beta}\Big(\sum_{m=1}^M \omega_{km} F_m(y)\Big),
\end{align}

where the vector $w_1,..., w_K$ comprises the beta mixture weights, $\alpha_1,..., \alpha_K$ and $\beta_1,..., \beta_K$ are beta calibration parameters, and for each beta component $\boldsymbol{\omega}_k=(\omega_{k1},..., \omega_{kM})$ comprises the beta component-specific set of component model weights. The pdf representation of the method is

\begin{align}
f_{\text{BM}_k}(y)=\sum_{k=1}^K w_k(\sum_{m=1}^M \omega_{km} f_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \omega_{km} F_m(y)\Big).
\end{align}

## Finite beta mixture with equally weighted ensemble  ($\text{EW-BM}_k$)

The $\text{EW-BM}_k$ uses a finite beta mixture combination formula to combine an equally-weighted ensemble as follows

\begin{align}
F_{\text{EW-BM}_k}(y)=\sum_{k=1}^K w_kB_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big),
\end{align}

where the vector $w_1,..., w_K$ comprises the beta mixture weights and $\alpha_1,..., \alpha_K$ and $\beta_1,..., \beta_K$ are beta calibration parameters.

\begin{align}
f_{\text{EW-BM}_k}(y)=\sum_{k=1}^K w_k(\sum_{m=1}^M \frac{1}{M} f_m(y)\Big)b_{\alpha,\beta}\Big(\sum_{m=1}^M \frac{1}{M} F_m(y)\Big).
\end{align}

# Cross Validation and Training Process

The 2016/2017, 2017/2018, 2018/2019 seasons are selected as the test seasons. For the mixture methods, $\text{BMC}_k$ and $\text{EW-BMC}_k$, the leave-one-season-out cross validation is used to select the number of beta mixture component $k$. For each test season, all the seasons preceding the test season are used for the leave-one-season-out cross validation (i.e., if there are $N$ seasons up until the test season, then $N-1$ seasons are used in the leave-one-season-out cross validation with $N-2$ seasons as training seasons and each one of the $N-1$ seasons is the validation season). The mean validation log scores are calculated across all  $N-1$ seasons, and one lowest $k$ with the mean validation log scores within one standard deviation of all methods' mean validation log scores is selected for $\text{BMC}_k$ and $\text{EW-BMC}_k$. We did not simply pick the methods with lowest mean validation log scores in order to take into account model complexity (a less complex model is preferred with the mean validation log score is not much worse compared to a more complex model).

The four methods (BLP, EW-BLP, selected $\text{BMC}_k$, selected $\text{EW-BMC}_k$) are trained all the seasons preceding each test season and the mean log scores are calculated across all week,locations, training seasons. For EW and TLP, the first does not need training and we use the FSN-TW ensemble's estimated parameters for the latter. The estimated parameters are applied to build the ensembles for the test season and the mean log scores are calculated across all week and locations for all three test seasons. 

# Log scores

## Cross-validated mean log scores for $\text{BMC}_k$ and $\text{EW-BMC}_k$

```{r}
path <- getwd()
seasons <- c("2010/2011","2011/2012","2012/2013","2013/2014","2014/2015","2015/2016",
             "2016/2017","2017/2018","2018/2019")
test_s <- seasons[7:9]
tars  <- sapply(1:4, function(x) paste0(x," wk ahead"))
source(paste0(path,"/BLPwork_functions_app/tables_plots_functions.R"))
source(paste0(path,"/BLPwork_functions_app/functions_app.R"))
## get data
cv_scores1 <- read.csv(paste0(path,"/ensembles/pit_ls_frame/cv/tables/sea17.csv")) %>%
  dplyr::group_by(model_name,target) %>%
  dplyr::mutate(mtls=mean(mean_ls_train),
                mtest=mean(mean_ls_valid))%>%
  dplyr::select(-"mean_ls_train",-"mean_ls_valid",-"valids",-"valid_season") %>%
  dplyr::arrange(target) %>%
  distinct()
cv_scores2 <- read.csv(paste0(path,"/ensembles/pit_ls_frame/cv/tables/sea18.csv")) %>%
  dplyr::group_by(model_name,target) %>%
  dplyr::mutate(mtls=mean(mean_ls_train),
                mtest=mean(mean_ls_valid))%>%
  dplyr::select(-"mean_ls_train",-"mean_ls_valid",-"valids",-"valid_season") %>%
  dplyr::arrange(target) %>%
  distinct()
cv_scores3 <- read.csv(paste0(path,"/ensembles/pit_ls_frame/cv/tables/sea19.csv")) %>%
  dplyr::group_by(model_name,target) %>%
  dplyr::mutate(mtls=mean(mean_ls_train),
                mtest=mean(mean_ls_valid))%>%
  dplyr::select(-"mean_ls_train",-"mean_ls_valid",-"valids",-"valid_season") %>%
  dplyr::arrange(target) %>%
  distinct()
colnames(cv_scores1) <- c("Target","Model Name","Mean train log score","Mean validation log score")
colnames(cv_scores2) <- c("Target","Model Name","Mean train log score","Mean validation log score")
colnames(cv_scores3) <- c("Target","Model Name","Mean train log score","Mean validation log score")
```

```{r}
# table
knitr::kable(cv_scores1,caption = paste0("Test season ",test_s[1]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BMC2 and EM-BMC2 have the lowest mean validation log scores and are selected for training for all targets. 
\item For 1 and 2 week ahead targets, $\text{BMC}_k$ outperform $\text{EW-BMC}_k$ with the same $k$, but the reverse is true for 3 and 4 week ahead targets.
\item We start to see some evidence of overfitting in $\text{BMC}_k$ compared to $\text{EW-BMC}_k$ for 3 and 4 week ahead targets ($\text{BMC}_k$'s train log scores are higher, but the mean validation log scores are worse than $\text{EW-BMC}_k$'s).
\end{itemize}

\newpage

```{r}
knitr::kable(cv_scores2,caption = paste0("Test season ",test_s[2]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BMC2 and EM-BMC2 have the lowest mean validation log scores and are selected for training for all targets. 
\item For 1 and 2 week ahead targets, $\text{BMC}_k$ outperform $\text{EW-BMC}_k$ with the same $k$, but the reverse is true for 3 and 4 week ahead targets.
\item We start to see some evidence of overfitting in $\text{BMC}_k$ compared to $\text{EW-BMC}_k$ for 3 and 4 week ahead targets ($\text{BMC}_k$'s train log scores are higher, but the mean validation log scores are worse than $\text{EW-BMC}_k$'s).
\end{itemize}

\newpage

```{r}
knitr::kable(cv_scores3,caption = paste0("Test season ",test_s[3]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BMC2 is selected for all targets. EM-BMC2 are selected for training for 1-3 week ahead targets and EM-BMC5 is selected for the 4 week ahead target.
\item Similar to the previous test season, for 1 and 2 week ahead targets, $\text{BMC}_k$ outperform $\text{EW-BMC}_k$ with the same $k$, but the reverse is true for 3 and 4 week ahead targets. We also start to see some evidence of overfitting in $\text{BMC}_k$ compared to $\text{EW-BMC}_k$ for 3 and 4 week ahead targets ($\text{BMC}_k$'s train log scores are higher, but the mean validation log scores are worse than $\text{EW-BMC}_k$'s).
\end{itemize}

\newpage

## Mean train and test log scores

```{r}
## get data
final_ls1 <- read.csv(paste0(path,"/ensembles/pit_ls_frame/train_test/ls_fsn_frame.csv")) %>%
  dplyr::mutate(model_name=ifelse(model_name=="BLP","TLP","EW"))
final_ls2  <- read.csv(paste0(path,"/ensembles/pit_ls_frame/train_test/ls_tw_frame.csv")) %>%
  dplyr::filter(!is.na(mean_ls_test)) 
final_ls <- rbind(final_ls1,final_ls2)
# table
for(i in 1:3){
  final_ls2 <- final_ls %>%
    dplyr::filter(test_season==test_s[i]) %>%
    dplyr::arrange(target) 
  # %>%
    # #rowwise %>%
    # dplyr::mutate(model_name = ifelse(model_name=="TLP","FSNetwork-TW",
    #                 ifelse(model_name=="EW","EW-TLP",
    #                        ifelse(model_name=="EW_BLP","EW-BLP",
    #                               ifelse(model_name=="BMC2",expression("BMC"[2]),
    #                                      ifelse(model_name=="EW_BMC2",expression("EW-BMC"[2]),
    #                                             ifelse(model_name=="EW_BMC5",expression("EW-BMC"[5]),
    #                                                    expression("EW-BMC"[3])))))))
               #   )
  
  colnames(final_ls2) <- c("Target","Model Name","Train log score","Test log score","Test Season")
  assign(paste0("tab",i),final_ls2)
}
```

### Mean train and test log scores by target-season

```{r}
kable(tab1,caption = paste0("Test season ",test_s[1]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BLP outperforms other methods for 1-3 week ahead targets, exceot for the 4 week ahead target where TLP outperforms.

\end{itemize}

\newpage

```{r}
kable(tab2,caption = paste0("Test season ",test_s[2]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BLP outperforms other methods for 1-2 week ahead targets, BMC2 outperforms for the 3 week ahead target, and TLP outperform for the 4 week ahead target.
\end{itemize}

\newpage

```{r}
kable(tab3,caption = paste0("Test season ",test_s[3]),booktabs=T,linesep = "",digits=3) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

\begin{itemize}
\item BMC2 outperforms for the 1 and 3-4 week ahead targets and BLP outperforms other methods for the 2 week ahead target.
\end{itemize}

\newpage 

# PIT Histograms

```{r, include=FALSE}
for(j in 1:4){
  for(i in 1:3){
    fs <- read.csv(paste0(path,"/ensembles/pit_ls_frame/train_test/EW_TLP/",16+i,"/target",j,".csv")) %>%
      dplyr::mutate(model_name=ifelse(model_name=="BLP","TLP","EW"))
    assign(paste0("sea",16+i,"_tar",j),
           rbind(read.csv(paste0(path,"/ensembles/pit_ls_frame/train_test/",16+i,"/target",j,".csv")),fs)
           ) 
  }
}

ensemble_list <- c("TLP","EW","BLP","EW_BLP","BMC2","EW_BMC2")
ensemble_list2 <- c("TLP","EW","BLP","EW_BLP","BMC2","EW_BMC5")
ensemble_list3 <- c("TLP","EW","BLP","EW_BLP","BMC2","EW_BMC3")
```

## Test season 2016/2017

### 1 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar1,tars[1],seasons[1:6], ens)
}
```

\newpage

```{r fig1,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2016/2017"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar1,tars[1],test_s[1], ens)
}
```

\begin{itemize}
\item There is evidence of bias in the PIT histograms in both the training and test seasons. The BLP, which outperforms other methods, has a more uniform PIT histogram in the test season.
\item The training PIT histograms for BMC2, EW-BLP, and EW-BMC2 look more uniform compared to that of BLP, maybe overfitting?
\end{itemize}

\newpage

### 2 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar2,tars[2],seasons[1:6], ens)
}
```

\newpage

```{r fig2,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2016/2017"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar2,tars[2],test_s[1], ens)
}
```

\begin{itemize}
\item There is evidence of some bias in the PIT histograms in both the training and test seasons but less than the previous target. The BLP, which outperforms other methods, has a more uniform PIT histogram in the test season.
\end{itemize}

\newpage

### 3 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar3,tars[3],seasons[1:6], ens)
}
```

\newpage


```{r fig3,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2016/2017"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot2(sea17_tar3,tars[3],test_s[1], ens,c(0,2.5))
}
```

\begin{itemize}
\item There is evidence of some bias in the PIT histograms in both the training and test seasons. The BLP, which outperforms other methods, has a more uniform PIT histogram in the test season.
\item There might be some overfitting going on, the train PIT histograms look a lot better than the test PIT histograms.
\end{itemize}

\newpage

### 4 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
ens_names <- unique(sea17_tar4$model_name)
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar4,tars[4],seasons[1:6], ens)
}
```

\newpage


```{r fig4,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2016/2017"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea17_tar4,tars[4],test_s[1], ens)
}
```

\begin{itemize}
\item There is evidence of a little bias in the PIT histograms in both the training and test seasons. 
\item The BLP, EW-BLP, BMC2, and EW-BMC2 are relatively well-calibrated in the training seasons which outperforms other methods, has a more uniform PIT histogram in the test season.
\item TLP outperforms other methods in terms of mean test log score, but the beta methods seem to have more uniform PIT histograms.
\end{itemize}

\newpage

## Test season 2017/2018

### 1 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea18_tar1,tars[1],seasons[1:7], ens)
}
```

\newpage

```{r fig5,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2017/2018"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot2(sea18_tar1,tars[1],test_s[2], ens,c(0,2.5))
}
```

\begin{itemize}
\item There is evidence of some bias in the PIT histograms in both the training and test seasons. The BLP, which outperforms other methods, has a more uniform PIT histogram in the test season, but it is not very calibrated.
\end{itemize}

\newpage

### 2 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea18_tar2,tars[2],seasons[1:7], ens)
}
```

\newpage

```{r fig6,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2017/2018"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot2(sea18_tar2,tars[2],test_s[2], ens,c(0,3))
}
```

\begin{itemize}
\item There is evidence of some bias in the PIT histograms in both the training and test seasons. Overall the PIT histograms for the beta methods do not look uniform for the test season, despite being relatively well calibrated for the training seasons
\item The BLP, which outperforms other methods, does not seem to be more calibrated than the TLP in the test season.
\end{itemize}

\newpage

### 3 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea18_tar3,tars[3],seasons[1:7], ens)
}
```

\newpage

```{r fig7,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2017/2018"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot2(sea18_tar3,tars[3],test_s[2], ens,c(0,3))
}
```

\begin{itemize}
\item We have a similar situation here as in the previous target of the same year, but the tail calibration is much worse.
\item BMC2 is the best performing method in terms of mean log score, but again it does not seem more calibrated than TLP (or worse even).
\end{itemize}

\newpage

### 4 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
ens_names <- unique(sea18_tar4$model_name)
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list2){
  pitplot(sea18_tar4,tars[4],seasons[1:7], ens)
}
```

\newpage


```{r fig8,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2017/2018"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list2){
  pitplot2(sea18_tar4,tars[4],test_s[2], ens,c(0,3))
}
```

\begin{itemize}
\item PIT histograms for the training season look well calibrated, but very uncalibrated for the test seasons.
\item TLP outperforms other methods here in terms of log score, and the PIT histograms agree.
\item For this season, it is possible the poor calibration is a result from training seasons being very different from the test season (bad flu season in 2017/2018), so we have a lot of overfitting. This phenomenon is more apparent for 3-4 week ahead targets.
\end{itemize}

\newpage

## Test season 2018/2019

### 1 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea19_tar1,tars[1],seasons[1:8], ens)
}
```

\newpage

```{r fig9,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2018/2019"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea19_tar1,tars[1],test_s[3], ens)
}
```


\begin{itemize}
\item There is evidence of some bias in the PIT histograms in the training seasons, but look more calibrated for the test season.
\item The BMC2, which outperforms other methods, does not seem to have a more uniform PIT histogram in the test season compared to other beta methods.
\end{itemize}

\newpage

### 2 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea19_tar2,tars[2],seasons[1:8], ens)
}
```

\newpage

```{r fig10,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2018/2019"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea19_tar2,tars[2],test_s[3], ens)
}
```

\begin{itemize}
\item The PIT histograms in the training and test seasons look similar for the beta methods. The BLP, which outperforms other methods, has a more uniform PIT histogram in the test season.
\item There is evidence of bias.
\end{itemize}

\newpage

### 3 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
ens_names <- unique(sea19_tar3$model_name)
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list3){
  pitplot(sea19_tar3,tars[3],seasons[1:8], ens)
}
```

\newpage

```{r fig11,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2018/2019"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list3){
  pitplot(sea19_tar3,tars[3],test_s[3], ens)
}
```

\begin{itemize}
\item There is evidence of some bias in the PIT histograms in the test season, but look more calibrated for the training seasons (no surprise here).
\item The BMC2, which outperforms other methods, does not seem to have a more uniform PIT histogram in the test season compared to other beta methods.
\end{itemize}

\newpage

### 4 Week Ahead

```{r,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Training Seasons"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot(sea19_tar4,tars[4],seasons[1:8], ens)
}
```

\newpage

```{r fig12,fig.align="center",fig.pos='H', fig.height=5,fig.cap="PIT Histograms for Test Season 2018/2019"}
par(mfrow = c(2, 3),mai = c(0.4, 0.5, 0.5, 0.4))
# ensembles
for (ens in ensemble_list){
  pitplot2(sea19_tar4,tars[4],test_s[3], ens,c(0,2.5))
}
```

\begin{itemize}
\item We see a lot bias in the PIT histograms in test seasons, especially for the equally-weighted beta methods, despite the PIT histograms looking well-calibrated for the training seasons.
\item The BMC2, which outperforms other methods, has a more uniform PIT histogram in the test season compared to other methods. However, these don't look well-calibrated overall.
\end{itemize}

# Estimated Parameters

## Test season 2016/2017

### 1 Week Ahead

```{r}
# order model
table1 <- make_sim_table(ensemble_list,1,tars,1,test_s,path)
kable(table1[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table1[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table1[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table1[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table1[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 2 Week Ahead

```{r}
# order model
table2 <- make_sim_table(ensemble_list,2,tars,1,test_s,path)
kable(table2[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table2[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table2[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table2[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table2[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```


### 3 Week Ahead

```{r}
# order model
table3 <- make_sim_table(ensemble_list,3,tars,1,test_s,path)
kable(table3[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table3[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table3[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table3[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table3[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 4 Week Ahead

```{r}
# order model
table4 <- make_sim_table(ensemble_list,4,tars,1,test_s,path)
kable(table4[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table4[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table4[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table4[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table4[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

\newpage

## Test season 2017/2018

### 1 Week Ahead

```{r}
# order model
table5 <- make_sim_table(ensemble_list,1,tars,2,test_s,path)
kable(table5[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table5[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table5[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table5[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table5[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 2 Week Ahead

```{r}
# order model
table6 <- make_sim_table(ensemble_list,2,tars,2,test_s,path)
kable(table6[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table6[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table6[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table6[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table6[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 3 Week Ahead

```{r}
# order model
table7 <- make_sim_table(ensemble_list,3,tars,2,test_s,path)
kable(table7[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table7[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table7[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table7[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table7[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 4 Week Ahead

```{r}
# order model
table8 <- make_sim_table(ensemble_list2,4,tars,2,test_s,path)
kable(table8[,1:6], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,7:16)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,17:29)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,30:43)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,44:56)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,57:70)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,71:83)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,84:97)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,98:110)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,111:124)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,125:137)], escape = FALSE,linesep = "",booktabs=T)
kable(table8[,c(1,138:151)], escape = FALSE,linesep = "",booktabs=T)
```

\newpage

## Transformation Examples

I picked week 6, US National, 2016/17 (one of the train seasons) and 2017/18 (test season). What we should note about the beta mixture PDFs is that we make a mixture of beta CDFs first, then convert to PDFs.

```{r}
source(paste0(path,"/BLPwork_functions_app/functions_app.R"))
# final dis
some_files <- list.files(paste0(path,"/ensembles/final_ensembles/18/target4/"),
                         full.names=TRUE, recursive = TRUE)
tmp <- lapply(some_files, FUN=read.csv)
frame <- do.call(rbind.data.frame, tmp)
frame <- frame[,c(1:6,8,7,9)]
# combine
final_test <- frame %>%
  dplyr::filter(season=="2017/2018",calendar_week==6, model_name=="BMC2", location=="US National")
final_train <- frame %>%
  dplyr::filter(season=="2016/2017",calendar_week==6, model_name=="BMC2", location=="US National")

# make transformations
load(paste0(path,"/train_params/final_train/target4_sea18_params.rda"))
pars_use <- unlist(get(paste0("target4_sea18_params"))[2])
test_f <- make_tran_pdfs("BMC2",pars_use, 2, 18, "4 wk ahead","US National",6)
train_f <- make_tran_pdfs("BMC2",pars_use, 2, 17, "4 wk ahead","US National",6)
bin <- seq(0,13,0.1)
```


```{r,fig.align="center",fig.pos='H', fig.height=8}
par(mfrow = c(3, 2),mai = c(0.7, 0.6, 0.3, 0.1))
## component 1 train
plot(train_f[[1]][,1]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",ylab="CDF",
     main="2016/17 (train) - week 6 - US")
lines(train_f[[2]][,1]~bin,lty=2)
legend("bottomright", inset=.05, lty=c(1,2),
       c("H for Beta comp 1","Beta CDF 1"))
## component 1 test
plot(test_f[[1]][,1]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",ylab="CDF",
     main="2017/18 (test) - week 6 - US")
lines(test_f[[2]][,1]~bin,lty=2)
legend("bottomright", inset=.05, lty=c(1,2),
       c("H for Beta comp 1","Beta CDF 1"))

## component 2 train
plot(train_f[[1]][,2]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",ylab="CDF",
     main="2016/17 (train) - week 6 - US",col=2)
lines(train_f[[2]][,2]~bin,lty=2,col=2)
legend("bottomright", inset=.05, lty=c(1,2),
       c("H for Beta comp 2","Beta CDF 2"), col=c(2,2))

## component 2 test
plot(test_f[[1]][,2]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",ylab="CDF",
     main="2017/18 (test) - week 6 - US",col=2)
lines(test_f[[2]][,2]~bin,lty=2,col=2)
legend("bottomright", inset=.05, lty=c(1,2),
       c("H for Beta comp 2","Beta CDF 2"), col=c(2,2))

## combine train
plot(train_f[[2]][,1]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",
     ylab="CDF",main="2016/17 (train) - week 6 - US")
#lines((train_f[[2]][,1]*train_f[[3]][1])~bin,lty=2)
#  comp 2
lines(train_f[[2]][,2]~bin,type="s",col=2)
#lines((train_f[[2]][,2]*train_f[[3]][2])~bin,lty=2,col=2)
lines(final_train$cdf_vals~bin,type="s",col=3)
legend("bottomright", inset=.05, lty=c(1,2,1,2,1),
       c("Beta Train CDF 1","Beta Train CDF 2","Final Train CDF"), col=1:3)
# legend("bottomright", inset=.05, lty=c(1,2,1,2,1),
#        c("Beta Train CDF 1","Beta Train CDF 1*weight",
#          "Beta Train CDF 2","Beta Train CDF 2*weight","Final Train CDF"), col=c(1,1,2,2,3))

## combine test
plot(test_f[[2]][,1]~bin,type="s",xlim=c(0,13),ylim=c(0,1),xlab="bin",
     ylab="CDF",main="2017/18 (test) - week 6 - US")
#lines((test_f[[2]][,2]*test_f[[3]][1])~bin,lty=2)
#  comp 2
lines(test_f[[2]][,2]~bin,type="s",col=2)
#lines((test_f[[2]][,2]*test_f[[3]][2])~bin,lty=2,col=2)
lines(final_test$cdf_vals~bin,type="s",col=3)
legend("bottomright", inset=.05, lty=c(1,2,1,2,1),
       c("Beta Test CDF 1","Beta Test CDF 2","Final Test CDF"), col=1:3)
# legend("bottomright", inset=.05, lty=c(1,2,1,2,1),
#        c("Beta Test CDF 1","Beta Test CDF 1*weight",
#          "Beta Test CDF 2","Beta Test CDF 2*weight","Final Test CDF"), col=c(1,1,2,2,3))
# ## PDF
# plot(cdf_to_pdf(train_f[[2]][,1]*train_f[[3]][1])~bin,type="s",xlim=c(0,13),ylim=c(0,0.12),xlab="bin",
#      ylab="Beta component pdf",main="2016/17 (train) - week 6 - US")
# lines(cdf_to_pdf(train_f[[2]][,2]*train_f[[3]][2])~bin,type="s",col=2)
# lines(final_train$value~bin,type="s",col=3)
# legend("topright", inset=.05, lty=c(1,1,1),
#        c("Beta PDF 1*w1","Beta PDF 2*w2","pdf"), col=1:3)
# 
# ## PDF
# plot(cdf_to_pdf(test_f[[2]][,1]*test_f[[3]][1])~bin,type="s",xlim=c(0,13),ylim=c(0,0.1),xlab="bin",
#      ylab="Beta component pdf",main="2017/18 (test) - week 6 - US")
# lines(cdf_to_pdf(test_f[[2]][,2]*test_f[[3]][2])~bin,type="s",col=2)
# lines(final_test$value~bin,type="s",col=3)
# legend("topright", inset=.05, lty=c(1,1,1),
#        c("Beta PDF 1*w1","Beta PDF 2*w2","pdf"), col=1:3)

```

\newpage

## Test season 2018/2019

### 1 Week Ahead

```{r}
# order model
table9 <- make_sim_table(ensemble_list,1,tars,3,test_s,path)
kable(table9[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table9[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table9[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table9[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table9[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 2 Week Ahead

```{r}
# order model
table10 <- make_sim_table(ensemble_list,2,tars,3,test_s,path)
kable(table10[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table10[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table10[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table10[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table10[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

### 3 Week Ahead

```{r}
# order model
table11 <- make_sim_table(ensemble_list3,3,tars,3,test_s,path)
kable(table11[,1:10], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,11:23)], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,24:37)], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,38:50)], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,51:64)], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,65:77)], escape = FALSE,linesep = "",booktabs=T)
kable(table11[,c(1,78:91)], escape = FALSE,linesep = "",booktabs=T)
```

### 4 Week Ahead

```{r}
# order model
table12 <- make_sim_table(ensemble_list,4,tars,3,test_s,path)
kable(table12[,1:7], escape = FALSE,linesep = "",booktabs=T)
kable(table12[,c(1,8:20)], escape = FALSE,linesep = "",booktabs=T)
kable(table12[,c(1,21:34)], escape = FALSE,linesep = "",booktabs=T)
kable(table12[,c(1,35:47)], escape = FALSE,linesep = "",booktabs=T)
kable(table12[,c(1,48:61)], escape = FALSE,linesep = "",booktabs=T)
```

